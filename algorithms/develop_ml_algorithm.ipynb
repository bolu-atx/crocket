{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/bhsu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "from enum import Enum\n",
    "from itertools import cycle\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange, seed\n",
    "from os.path import join\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = '/home/b3arjuden/crocket/sql_data/PRODUCTION40'\n",
    "\n",
    "path = '/Users/bhsu/crypto/sql_data/PRODUCTION40'\n",
    "\n",
    "file = 'BTC-ETH.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = join(path, file)\n",
    "\n",
    "data = pd.read_csv(file_path, \n",
    "                   dtype={'time': str, 'buy_order': int, 'sell_order': int},\n",
    "                   converters={'price': Decimal,\n",
    "                               'wprice': Decimal,\n",
    "                               'base_volume': Decimal,\n",
    "                               'buy_volume': Decimal,\n",
    "                               'sell_volume': Decimal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_transform(df, n):\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    \n",
    "    nrows = n * floor(df.shape[0] / n)\n",
    "    df = df.iloc[:nrows, :]\n",
    "    \n",
    "    df1['time'] = df.loc[df.index[::n], 'time'].reset_index(drop=True)\n",
    "    df1['wprice'] = ((df.loc[:, 'wprice'] * df.loc[:, 'base_volume']).groupby(df.index // n).sum() / \n",
    "                     df.loc[:, 'base_volume'].groupby(df.index // n).sum()).apply(lambda x: float(x.quantize(Decimal(10) ** -8)))\n",
    "    df1['buy_volume'] = df.loc[:, 'buy_volume'].groupby(df.index // n).sum().apply(float)\n",
    "    df1['sell_volume'] = df.loc[:, 'sell_volume'].groupby(df.index // n).sum().apply(float)\n",
    "    df1['buy_order'] = df.loc[:, 'buy_order'].groupby(df.index // n).sum()\n",
    "    df1['sell_order'] = df.loc[:, 'sell_order'].groupby(df.index // n).sum()\n",
    "    \n",
    "    return df1\n",
    "\n",
    "def vectorize(array):\n",
    "    \n",
    "    return array.reshape(array.size, 1)\n",
    "\n",
    "def unvectorize(vector, x, y):\n",
    "    \n",
    "    return vector.reshape(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Time series parameters\n",
    "        self.num_actions = 3\n",
    "        \n",
    "        # Neural network parameters\n",
    "        self.discount_factor = 0.97\n",
    "        self.epsilon = 0.9\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.95\n",
    "        \n",
    "        # Replay memory parameters\n",
    "        self.replay_memory_size = 10000\n",
    "        self.batch_size = 128\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_actions, \n",
    "                 replay_memory,\n",
    "                 checkpoint_path=None):\n",
    "        \n",
    "        self.replay_memory = replay_memory\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        \n",
    "        self.count_states = 0\n",
    "        self.count_episodes = 0\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=30, input_dim=20, activation='relu'))\n",
    "        model.add(Dense(units=3, activation='softmax'))\n",
    "        \n",
    "        # Optimizer: adam, RMSProp\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def save(self):\n",
    "        \n",
    "        save_file = 'network_S{}_E{}.h5'.format(self.count_states, self.count_episodes)\n",
    "        model.save(join(self.checkpoint_path, save_file))\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        \n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        # TODO: load count_states and count_episodes from file\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \n",
    "        return [layer.get_weights() for layer in self.model.layers]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \n",
    "        for layer, new_weights in zip(self.model.layers, weights):\n",
    "            layer.set_weights(new_weights)\n",
    "    \n",
    "    def interpolate_weights(self, weights, interpolation_factor=0.001):\n",
    "        \n",
    "        for layer, new_weights in zip(self.model.layers, weights):\n",
    "            layer.set_weights([w1 * interpolation_factor + (1 - interpolation_factor) * w0 for w0, w1 in zip(layer.get_weights(), new_weights)])\n",
    "    \n",
    "    # TODO: add initialization function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        \n",
    "        self.num_actions = parameters.num_actions\n",
    "        self.discount_factor = parameters.discount_factor\n",
    "        self.epsilon = parameters.epsilon\n",
    "        self.epsilon_min = parameters.epsilon_min\n",
    "        self.epsilon_decay = parameters.epsilon_decay\n",
    "        \n",
    "    def initialize(self, model_path=None):\n",
    "        \n",
    "        if model_path:\n",
    "            pass\n",
    "            # TODO: implement load model from folder (network_weights, replay_memory, additional_params)\n",
    "        else:\n",
    "            self.replay_memory = ReplayMemory(size=parameters.replay_memory_size,\n",
    "                                          num_actions = self.num_actions,\n",
    "                                          discount_factor=self.discount_factor)\n",
    "        \n",
    "            self.train_model = NeuralNetwork(num_actions=self.num_actions,\n",
    "                                             replay_memory=self.replay_memory)\n",
    "\n",
    "            self.target_model = NeuralNetwork(num_actions=self.num_actions)\n",
    "            self.target_model.set_weights(self.train_model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        \n",
    "        values = self.target_model.predict(state)\n",
    "        \n",
    "        return np.argmax(values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def run(self, num_episodes=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, memory_size, state_shape, num_actions, discount_factor):\n",
    "        \n",
    "        self.memory_size = memory_size\n",
    "        size.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.state_shape = state_shape\n",
    "        \n",
    "        self.states = np.zeros(shape=[memory_size] + state_shape, dtype=np.float64)\n",
    "        self.states_next = np.zeros(shape=[memory_size] + state_shape, dtype=np.float64)\n",
    "        self.actions = np.zeros(shape=[memory_size, num_actions], dtype=np.int8)\n",
    "        self.rewards = np.zeros(shape=[memory_size, num_actions], dtype=np.float64)\n",
    "        \n",
    "        self.indexes = cycle(range(memory_size))\n",
    "        self.pointer = next(self.indexes)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state):\n",
    "        \n",
    "        k = self.pointer\n",
    "        \n",
    "        self.states[k] = state\n",
    "        self.actions[k] = action\n",
    "        self.rewards[k] = reward  # TODO: consider clipping\n",
    "        self.next_states[k] = next_state\n",
    "        \n",
    "        self.pointer = next(self.indexes)\n",
    "        \n",
    "    def random_batch(self, batch_size):\n",
    "        \n",
    "        idx = np.random.randint(size, size=batch_size)\n",
    "        \n",
    "        return self.states[idx], self.actions[idx], self.rewards[idx], self.states_next[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_data,\n",
    "                 steps=None):\n",
    "        \n",
    "        self.steps = steps\n",
    "        self._load(input_data)\n",
    "        \n",
    "        self.episode = {}\n",
    "        \n",
    "    def _load(self, input_data):\n",
    "        \n",
    "        self.data = input_data\n",
    "        \n",
    "        self.shape = input_data.shape\n",
    "        \n",
    "        if self.steps is None or self.steps > self.shape[0]:\n",
    "            self.steps = self.shape[0] - 1\n",
    "    \n",
    "    def _seed(self):\n",
    "        \n",
    "        seed()\n",
    "    \n",
    "    def _add_state(self):\n",
    "        \n",
    "        np.insert(self.data, self.shape[1], np.zeros(self.shape[1]), axis=1)\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self._seed()\n",
    "        self.episode['start'] = np.random.randint(0, self.shape[0] - self.steps)\n",
    "        self.episode['current_index'] = self.episode.get('start')\n",
    "        self.spisode['buy_price'] = 0\n",
    "        self.data[:, -1] = 0  # Reset buy state\n",
    "        \n",
    "        return self.data[self.episode.get('current_index')]\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        current_index = self.episode.get('current_index')\n",
    "        state = self.data[current_index]\n",
    "        next_state = self.data[current_index + 1]\n",
    "        \n",
    "        price = state[0]  # TODO: add correct index for current price\n",
    "        # Custom behaviors\n",
    "        # TODO: add only buy once behavior maybe (penalty on every buy/sell should train not to buy many times)\n",
    "        \n",
    "        reward = 0\n",
    "                \n",
    "        \n",
    "        \n",
    "        self.episode['current_index'] += 1\n",
    "        \n",
    "        return next_state, reward\n",
    "    \n",
    "class Action(Enum):\n",
    "\n",
    "    HOLD = 0\n",
    "    BUY = 1\n",
    "    SELL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = time_transform(data, 15)\n",
    "array = df.iloc[:,1:].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(df['sell_order'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize data \n",
    "# TODO: change normalization if necessary\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train = min_max_scaler.fit_transform(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPISODES = 1\n",
    "EPISODE_LENGTH = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Main loop\n",
    "\n",
    "parameters = Parameters()\n",
    "agent = Agent(parameters)\n",
    "env = Environment(steps=EPISODE_LENGTH)\n",
    "step = 0\n",
    "\n",
    "# Populate the replay memory with initial experiences\n",
    "state = env.reset()\n",
    "for i in range(parameters.replay_memory_size):\n",
    "    \n",
    "    action = agent.act(state)\n",
    "    next_state, reward = env.step(action)\n",
    "    agent.replay_memory.add(state, action, reward, next_state)\n",
    "    state = next_state\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for ii in range(EPISODE_LENGTH):\n",
    "        \n",
    "        \n",
    "        # Update the target network weights every X iterations\n",
    "        if total_t % update_target_estimator_every == 0:\n",
    "            copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "            print(\"\\nCopied model parameters to target network.\")\n",
    "                \n",
    "        action = agent.act(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        agent.replay_memory.add(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        if step % train_every_n_steps == 0:\n",
    "            samples = agent.replay_memory.random_batch(parameters.batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            q_values_next = agent.train_model.predict(sess, next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = agent.target_model.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + discount_factor * q_values_next_target[np.arange(batch_size), best_actions]\n",
    "\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = agent.train_model.update(states_batch, action_batch, targets_batch)\n",
    "            \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
