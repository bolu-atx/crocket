{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/bhsu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "from enum import Enum\n",
    "from itertools import cycle\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange, seed\n",
    "from os.path import join\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = '/home/b3arjuden/crocket/sql_data/PRODUCTION40'\n",
    "\n",
    "path = '/Users/bhsu/crypto/sql_data/PRODUCTION40'\n",
    "\n",
    "file = 'BTC-ETH.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = join(path, file)\n",
    "\n",
    "data = pd.read_csv(file_path, \n",
    "                   dtype={'time': str, 'buy_order': int, 'sell_order': int},\n",
    "                   converters={'price': Decimal,\n",
    "                               'wprice': Decimal,\n",
    "                               'base_volume': Decimal,\n",
    "                               'buy_volume': Decimal,\n",
    "                               'sell_volume': Decimal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_transform(df, n):\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    \n",
    "    nrows = n * floor(df.shape[0] / n)\n",
    "    df = df.iloc[:nrows, :]\n",
    "    \n",
    "    df1['time'] = df.loc[df.index[::n], 'time'].reset_index(drop=True)\n",
    "    df1['open'] = (df.loc[::n, 'wprice'].values)\n",
    "    df1['high'] = (df.loc[:, 'wprice'].groupby(df.index // n).max()) # TODO: estimate using 2 standard deviation from mean\n",
    "    df1['low'] = (df.loc[:, 'wprice'].groupby(df.index // n).min()) # TODO: estimate using 2 standard deciation from mean \n",
    "    df1['close'] = (df.loc[(n-1)::n, 'wprice'].values)\n",
    "    \n",
    "    df1['buy_volume'] = df.loc[:, 'buy_volume'].groupby(df.index // n).sum()\n",
    "    df1['sell_volume'] = df.loc[:, 'sell_volume'].groupby(df.index // n).sum()\n",
    "    df1['buy_order'] = df.loc[:, 'buy_order'].groupby(df.index // n).sum()\n",
    "    df1['sell_order'] = df.loc[:, 'sell_order'].groupby(df.index // n).sum()\n",
    "    \n",
    "    #df1['wprice'] = ((df.loc[:, 'wprice'] * df.loc[:, 'base_volume']).groupby(df.index // n).sum() / \n",
    "    #                 df.loc[:, 'base_volume'].groupby(df.index // n).sum()).apply(lambda x: float(x.quantize(Decimal(10) ** -8)))\n",
    "    \n",
    "    return df1\n",
    "\n",
    "def vectorize(array):\n",
    "    \n",
    "    return array.reshape(1, *array.shape)\n",
    "\n",
    "def unvectorize(vector, x, y):\n",
    "    \n",
    "    return vector.reshape(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_actions,\n",
    "                 checkpoint_path=None):\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        \n",
    "        self.count_states = 0\n",
    "        self.count_episodes = 0\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(9, input_shape=(8, 2), activation='tanh'))\n",
    "        \n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        \n",
    "        # Optimizer: adam, RMSProp\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def save(self):\n",
    "        \n",
    "        save_file = 'network_S{}_E{}.h5'.format(self.count_states, self.count_episodes)\n",
    "        model.save(join(self.checkpoint_path, save_file))\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        \n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        # TODO: load count_states and count_episodes from file\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \n",
    "        return [layer.get_weights() for layer in self.model.layers]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \n",
    "        for layer, new_weights in zip(self.model.layers, weights):\n",
    "            layer.set_weights(new_weights)\n",
    "    \n",
    "    def interpolate_weights(self, weights, interpolation_factor=0.001):\n",
    "        \n",
    "        for layer, new_weights in zip(self.model.layers, weights):\n",
    "            layer.set_weights([w1 * interpolation_factor + (1 - interpolation_factor) * w0 for w0, w1 in zip(layer.get_weights(), new_weights)])\n",
    "    \n",
    "    # TODO: add initialization function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Rules:\n",
    "    - Position = 0 -> HOLD, BUY\n",
    "    - Position => 1 -> HOLD, SELL\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        \n",
    "        self.num_actions = parameters.num_actions\n",
    "        self.discount_factor = parameters.discount_factor\n",
    "        self.epsilon = parameters.epsilon\n",
    "        self.epsilon_min = parameters.epsilon_min\n",
    "        self.epsilon_decay = parameters.epsilon_decay\n",
    "        self.priority_alpha = parameters.priority_alpha\n",
    "        self.batch_size = parameters.batch_size\n",
    "        \n",
    "        self.prioritized_memory = parameters.prioritized_memory\n",
    "        self.replay_capacity = parameters.replay_capacity\n",
    "        self.state_length = parameters.state_length\n",
    "        self.priority_epsilon = parameters.priority_epsilon\n",
    "        \n",
    "        self._initialize()\n",
    "        \n",
    "    def _initialize(self, model_path=None):\n",
    "        \n",
    "        self.train_network = NeuralNetwork(num_actions=self.num_actions)\n",
    "        self.target_network = NeuralNetwork(num_actions=self.num_actions)\n",
    "        \n",
    "        if model_path:\n",
    "            pass\n",
    "            # TODO: implement load model from folder (network_weights, replay_memory, additional_params)\n",
    "        else:\n",
    "            \n",
    "            if self.prioritized_memory:\n",
    "                self.replay_memory = PrioritizedReplayMemory(capacity=self.replay_capacity)\n",
    "            else:\n",
    "                self.replay_memory = ReplayMemory(capacity=self.replay_capacity,\n",
    "                                                  state_shape=self.state_length,\n",
    "                                                  num_actions=self.num_actions)\n",
    "\n",
    "            self.train_network.build()\n",
    "            self.target_network.build()\n",
    "            \n",
    "            self.target_network.set_weights(self.train_network.get_weights())\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        # no position -> actions allowed: HOLD, BUY\n",
    "        # active position -> HOLD, BUY, SELL\n",
    "        \n",
    "        print\n",
    "        buy_price = state[0, -1, 0]\n",
    "        \n",
    "        if buy_price == 0:\n",
    "            actions = [0, 1]\n",
    "        else:\n",
    "            actions = [0, 2]\n",
    "            \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return actions[np.random.randint(2)]\n",
    "        \n",
    "        values = self.train_network.model.predict(state)\n",
    "        \n",
    "        return actions[np.argmax(values[0][actions])]\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Replay options: 1) replay memory, 2) prioritized replay memory\n",
    "        \"\"\"\n",
    "        if self.prioritized_memory:\n",
    "            idx, priorities, experience = self.replay_memory.sample(self.batch_size)\n",
    "            \n",
    "            # Update transition priorities\n",
    "        else:\n",
    "            samples = self.replay_memory.random_batch(self.batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, indexes = map(np.array, samples)\n",
    "\n",
    "            q_values_next = self.train_network.model.predict(next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = self.target_network.model.predict(next_states_batch)\n",
    "\n",
    "            # SELL equivalent to end of episode\n",
    "            targets_batch = reward_batch + \\\n",
    "                (action_batch != 2).astype(int) * (self.discount_factor * np.repeat(q_values_next_target[np.arange(self.batch_size), best_actions].reshape(self.batch_size, 1), self.num_actions, axis=1))\n",
    "\n",
    "            targets_batch = targets_batch * np.stack([[1, 1, 0] if x[-1, -1] == 0 else [1, 0, 1] for x in states_batch])\n",
    "\n",
    "            # TODO: clip reward\n",
    "\n",
    "            # Update train model\n",
    "            self.train_network.model.fit(states_batch, targets_batch)\n",
    "\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def initialize_replay_memory(self, env, parameters, price_index):\n",
    "        \n",
    "        print('Initializing replay memory...')\n",
    "        \n",
    "        ii = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while (ii < parameters.replay_capacity):\n",
    "            \n",
    "            action = self.act(state)\n",
    "            next_state, reward, _ = env.step(action, price_index)\n",
    "            \n",
    "            if self.prioritized_memory:\n",
    "                self.replay_memory.add((state, action, reward, next_state), max(self.priority_max, self.priority_epsilon))\n",
    "            else:\n",
    "                self.replay_memory.add(state, action, reward, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "                \n",
    "            if env.episode.get('current_index') == env.steps:\n",
    "                state = env.reset()\n",
    "                \n",
    "            ii += 1\n",
    "            \n",
    "        print('Replay memory initialized.')\n",
    "        \n",
    "    def update_transition_priorities(self, states_batch, actions_batch, rewards_batch, targets_batch, indexes):\n",
    "        \n",
    "        for s0, a, r, t, i in zip(states_batch, actions_batch, rewards_batch, targets_batch, indexes):\n",
    "            \n",
    "            priority = (abs(self.train_network.model.predict(s0)[a] - t[a]) + self.priority_epsilon) ** self.priority_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    # TODO: use collections.deque\n",
    "        \n",
    "    def __init__(self, capacity, state_shape, num_actions):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.state_shape = state_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.states = np.zeros(shape=[capacity] + state_shape, dtype=np.float64)\n",
    "        self.states_next = np.zeros(shape=[capacity] + state_shape, dtype=np.float64)\n",
    "        self.actions = np.zeros(shape=[capacity, num_actions], dtype=np.int8)\n",
    "        self.rewards = np.zeros(shape=[capacity, num_actions], dtype=np.float64)\n",
    "        \n",
    "        self.indexes = cycle(range(capacity))\n",
    "        self.pointer = next(self.indexes)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state):\n",
    "        \n",
    "        k = self.pointer\n",
    "\n",
    "        self.states[k, :, :] = state\n",
    "        self.actions[k] = action\n",
    "        self.rewards[k] = reward  # TODO: consider clipping\n",
    "        self.states_next[k, :, :] = next_state\n",
    "        \n",
    "        self.pointer = next(self.indexes)\n",
    "        \n",
    "    def random_batch(self, batch_size):\n",
    "        \n",
    "        idx = np.random.randint(self.capacity, size=batch_size)\n",
    "        \n",
    "        return self.states[idx], self.actions[idx], self.rewards[idx], self.states_next[idx], idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrioritizedReplayMemory:\n",
    "    \"\"\"\n",
    "    SumTree.\n",
    "    Data: (state, action, reward, next_state)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2*capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        \n",
    "        self.pointer = 0\n",
    "    \n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "\n",
    "        if left >= self.tree_len:\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            right = left + 1\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, data, p):\n",
    "        idx = self.pointer + self.capacity - 1\n",
    "\n",
    "        self.data[self.pointer] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.pointer += 1\n",
    "        \n",
    "        if self.pointer >= self.capacity:\n",
    "            self.pointer = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        data_idx = idx - self.capacity + 1\n",
    "\n",
    "        return idx, self.tree[idx], self.data[data_idx]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_idx = [None] * batch_size\n",
    "        batch_priorities = [None] * batch_size\n",
    "        batch = [None] * batch_size\n",
    "        segment = self.total() / batch_size\n",
    "\n",
    "        a = [segment*i for i in range(batch_size)]\n",
    "        b = [segment * (i+1) for i in range(batch_size)]\n",
    "        s = np.random.uniform(a, b)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            (batch_idx[i], batch_priorities[i], batch[i]) = self.get(s[i])\n",
    "\n",
    "        return batch_idx, batch_priorities, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_data,\n",
    "                 steps=None):\n",
    "        \n",
    "        self.steps = steps\n",
    "        self._load(input_data)\n",
    "        \n",
    "        self.episode = {}\n",
    "        \n",
    "    def _load(self, input_data):\n",
    "        \n",
    "        self.data = input_data\n",
    "        \n",
    "        self.shape = input_data.shape\n",
    "        \n",
    "        if self.steps is None or self.steps > self.shape[0]:\n",
    "            self.steps = self.shape[0] - 1\n",
    "    \n",
    "    def _seed(self):\n",
    "        \n",
    "        seed()\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self._seed()\n",
    "        self.episode['start'] = np.random.randint(0, self.shape[0] - self.steps)\n",
    "        self.episode['current_index'] = self.episode.get('start')\n",
    "        self.data[:, :, -1] = 0  # Reset buy state\n",
    "        \n",
    "        return vectorize(self.data[self.episode.get('current_index')])\n",
    "        \n",
    "    def step(self, action, price_index):\n",
    "        \n",
    "        current_index = self.episode.get('current_index')\n",
    "        state = vectorize(self.data[current_index])\n",
    "        next_state = vectorize(self.data[current_index + 1])\n",
    "\n",
    "        price = state[0, price_index, 0]\n",
    "        buy_price = state[0, -1, 0]\n",
    "        \n",
    "        if action == 1:\n",
    "            next_state[0, -1] = price\n",
    "            reward = [0, price * -2, 0]\n",
    "        elif action == 2:\n",
    "            next_state[0, -1] = 0\n",
    "            margin = price / buy_price\n",
    "            reward = [0, 0, (margin if margin > 1 else -margin) * 2]\n",
    "        else:\n",
    "            # TODO: set reward to difference between buy_price and current_price if position held, but smaller magnitude than buying or selling\n",
    "            next_state[0, -1] = buy_price\n",
    "            reward = [0, 0, 0]\n",
    "        \n",
    "        self.episode['current_index'] += 1\n",
    "        \n",
    "        return next_state, reward, current_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "\n",
    "    HOLD = 0\n",
    "    BUY = 1\n",
    "    SELL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Summary:\n",
    "    \n",
    "    def __init__(self, parameters=None):\n",
    "        \n",
    "        self.episodes = []\n",
    "        self.params = parameters\n",
    "        \n",
    "    def create(self, actions, train_reward, actual_reward, start, end, epsilon_end):\n",
    "        \n",
    "        # TODO: add epsilon values\n",
    "        \n",
    "        self.episodes.append({\n",
    "            'actions': actions,\n",
    "            'train_reward': train_reward,\n",
    "            'actual_reward': actual_reward,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'epsilon_end': epsilon_end\n",
    "        })\n",
    "        \n",
    "    def summarize_all(self):\n",
    "        \n",
    "        num_episodes = len(self.episodes)\n",
    "        train_reward = np.zeros(num_episodes)\n",
    "        actual_reward = np.zeros(num_episodes)\n",
    "        epsilon_end = np.zeros(num_episodes)\n",
    "        \n",
    "        for ii in range(num_episodes):\n",
    "            train_reward[ii], actual_reward[ii], epsilon_end[ii] = (self.episodes[ii][x] for x in ['train_reward', 'actual_reward', 'epsilon_end'])\n",
    "        \n",
    "        # TODO: add plot here\n",
    "        \n",
    "        return train_reward, actual_reward, epsilon_end\n",
    "    \n",
    "    def summarize_episode(self, episode_num):\n",
    "        \n",
    "        episode_summary = self.episodes[episode_num]\n",
    "        \n",
    "        return episode_summary\n",
    "    \n",
    "    def get_price(self, price, episode_num):\n",
    "        \n",
    "        start = self.episodes[episode_num].get('start')\n",
    "        end = self.episodes[episode_num].get('end')\n",
    "        \n",
    "        return price[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data processing parameters\n",
    "n = 15\n",
    "m = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "x = np.linspace(-np.pi, np.pi, 100)\n",
    "array = np.sin(x)\n",
    "\n",
    "price_state_index = -2\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "normalized_array = min_max_scaler.fit_transform(array.reshape(-1, 1))\n",
    "\n",
    "# Build states\n",
    "states = np.array([normalized_array[x:x+m, :] for x in range(normalized_array.shape[0] - m + 1)])\n",
    "\n",
    "# Add state for position held\n",
    "states = np.insert(states, states.shape[2], 0, axis=2)\n",
    "\n",
    "# Build prices\n",
    "prices = list(map(float, array[(m-1):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine data\n",
    "df = time_transform(data.iloc[:200*15,:], n)\n",
    "array = df.iloc[:,1:].as_matrix()\n",
    "\n",
    "# Get close price column index\n",
    "price_df_index = df.columns.get_loc('close')\n",
    "price_array_index = price_df_index-1\n",
    "price_state_index = m*(price_array_index+1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize data \n",
    "# TODO: change normalization if necessary\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "normalized_array = min_max_scaler.fit_transform(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build states\n",
    "states = np.array([normalized_array[x:x+m, :] for x in range(normalized_array.shape[0] - m + 1)])\n",
    "\n",
    "# Add state for position\n",
    "states = np.insert(states, states.shape[2], 0, axis=2)\n",
    "\n",
    "# Build prices\n",
    "prices = list(map(float, array[(m-1):, price_array_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Time series parameters\n",
    "        self.num_actions = 3\n",
    "        \n",
    "        # Neural network parameters\n",
    "        self.discount_factor = 0.97\n",
    "        self.epsilon = 0.99\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.98\n",
    "        \n",
    "        # Replay memory parameters\n",
    "        self.replay_capacity = 10000\n",
    "        self.batch_size = 100\n",
    "        self.priority_epsilon = 1e-6\n",
    "        self.priority_alpha = 2\n",
    "        self.prioritized_memory = False\n",
    "        \n",
    "        # Training parameters\n",
    "        self.episodes = 100\n",
    "        self.episode_length = 92\n",
    "        self.update_target_weights_step_size = 100\n",
    "        self.train_network_step_size = 100\n",
    "        self.interpolation_factor = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing replay memory...\n",
      "Replay memory initialized.\n",
      "Step 0: Updating target Q network weights with latest Q network weights.\n",
      "Step 0: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.9297\n",
      "Step 100: Updating target Q network weights with latest Q network weights.\n",
      "Step 100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 645us/step - loss: 0.4892\n",
      "Step 200: Updating target Q network weights with latest Q network weights.\n",
      "Step 200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 509us/step - loss: 0.9285\n",
      "Step 300: Updating target Q network weights with latest Q network weights.\n",
      "Step 300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 526us/step - loss: 0.9455\n",
      "Step 400: Updating target Q network weights with latest Q network weights.\n",
      "Step 400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 614us/step - loss: 0.5101\n",
      "Step 500: Updating target Q network weights with latest Q network weights.\n",
      "Step 500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 738us/step - loss: 1.2486\n",
      "Step 600: Updating target Q network weights with latest Q network weights.\n",
      "Step 600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 531us/step - loss: 0.4788\n",
      "Step 700: Updating target Q network weights with latest Q network weights.\n",
      "Step 700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 467us/step - loss: 2.1963\n",
      "Step 800: Updating target Q network weights with latest Q network weights.\n",
      "Step 800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 527us/step - loss: 2.5485\n",
      "Step 900: Updating target Q network weights with latest Q network weights.\n",
      "Step 900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 534us/step - loss: 0.4541\n",
      "Step 1000: Updating target Q network weights with latest Q network weights.\n",
      "Step 1000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 482us/step - loss: 1.4905\n",
      "Step 1100: Updating target Q network weights with latest Q network weights.\n",
      "Step 1100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 546us/step - loss: 2.9322\n",
      "Step 1200: Updating target Q network weights with latest Q network weights.\n",
      "Step 1200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 443us/step - loss: 1.5871\n",
      "Step 1300: Updating target Q network weights with latest Q network weights.\n",
      "Step 1300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 441us/step - loss: 0.4906\n",
      "Step 1400: Updating target Q network weights with latest Q network weights.\n",
      "Step 1400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.5097\n",
      "Step 1500: Updating target Q network weights with latest Q network weights.\n",
      "Step 1500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 459us/step - loss: 1.3480\n",
      "Step 1600: Updating target Q network weights with latest Q network weights.\n",
      "Step 1600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 435us/step - loss: 1.4739\n",
      "Step 1700: Updating target Q network weights with latest Q network weights.\n",
      "Step 1700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 357us/step - loss: 0.6324\n",
      "Step 1800: Updating target Q network weights with latest Q network weights.\n",
      "Step 1800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 505us/step - loss: 4.8716\n",
      "Step 1900: Updating target Q network weights with latest Q network weights.\n",
      "Step 1900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 347us/step - loss: 0.6989\n",
      "Step 2000: Updating target Q network weights with latest Q network weights.\n",
      "Step 2000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 345us/step - loss: 0.6194\n",
      "Step 2100: Updating target Q network weights with latest Q network weights.\n",
      "Step 2100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 368us/step - loss: 0.6271\n",
      "Step 2200: Updating target Q network weights with latest Q network weights.\n",
      "Step 2200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 487us/step - loss: 0.3601\n",
      "Step 2300: Updating target Q network weights with latest Q network weights.\n",
      "Step 2300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 550us/step - loss: 2.8480\n",
      "Step 2400: Updating target Q network weights with latest Q network weights.\n",
      "Step 2400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 455us/step - loss: 2.6996\n",
      "Step 2500: Updating target Q network weights with latest Q network weights.\n",
      "Step 2500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 385us/step - loss: 0.3801\n",
      "Step 2600: Updating target Q network weights with latest Q network weights.\n",
      "Step 2600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 328us/step - loss: 0.3816\n",
      "Step 2700: Updating target Q network weights with latest Q network weights.\n",
      "Step 2700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 365us/step - loss: 0.3152\n",
      "Step 2800: Updating target Q network weights with latest Q network weights.\n",
      "Step 2800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 407us/step - loss: 1.1788\n",
      "Step 2900: Updating target Q network weights with latest Q network weights.\n",
      "Step 2900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 447us/step - loss: 0.5307\n",
      "Step 3000: Updating target Q network weights with latest Q network weights.\n",
      "Step 3000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 591us/step - loss: 0.4389\n",
      "Step 3100: Updating target Q network weights with latest Q network weights.\n",
      "Step 3100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 583us/step - loss: 0.4856\n",
      "Step 3200: Updating target Q network weights with latest Q network weights.\n",
      "Step 3200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3627\n",
      "Step 3300: Updating target Q network weights with latest Q network weights.\n",
      "Step 3300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 610us/step - loss: 0.6127\n",
      "Step 3400: Updating target Q network weights with latest Q network weights.\n",
      "Step 3400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 492us/step - loss: 4.7278\n",
      "Step 3500: Updating target Q network weights with latest Q network weights.\n",
      "Step 3500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 530us/step - loss: 2.6697\n",
      "Step 3600: Updating target Q network weights with latest Q network weights.\n",
      "Step 3600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 440us/step - loss: 0.3555\n",
      "Step 3700: Updating target Q network weights with latest Q network weights.\n",
      "Step 3700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 376us/step - loss: 0.5099\n",
      "Step 3800: Updating target Q network weights with latest Q network weights.\n",
      "Step 3800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 353us/step - loss: 0.7242\n",
      "Step 3900: Updating target Q network weights with latest Q network weights.\n",
      "Step 3900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 470us/step - loss: 0.4329\n",
      "Step 4000: Updating target Q network weights with latest Q network weights.\n",
      "Step 4000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 675us/step - loss: 0.4777\n",
      "Step 4100: Updating target Q network weights with latest Q network weights.\n",
      "Step 4100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 372us/step - loss: 0.7446\n",
      "Step 4200: Updating target Q network weights with latest Q network weights.\n",
      "Step 4200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 535us/step - loss: 60.9225\n",
      "Step 4300: Updating target Q network weights with latest Q network weights.\n",
      "Step 4300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 390us/step - loss: 0.9718\n",
      "Step 4400: Updating target Q network weights with latest Q network weights.\n",
      "Step 4400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 473us/step - loss: 2.4695\n",
      "Step 4500: Updating target Q network weights with latest Q network weights.\n",
      "Step 4500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 418us/step - loss: 1.7776\n",
      "Step 4600: Updating target Q network weights with latest Q network weights.\n",
      "Step 4600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 475us/step - loss: 0.9990\n",
      "Step 4700: Updating target Q network weights with latest Q network weights.\n",
      "Step 4700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 549us/step - loss: 0.5670\n",
      "Step 4800: Updating target Q network weights with latest Q network weights.\n",
      "Step 4800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 382us/step - loss: 0.4483\n",
      "Step 4900: Updating target Q network weights with latest Q network weights.\n",
      "Step 4900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 431us/step - loss: 0.4652\n",
      "Step 5000: Updating target Q network weights with latest Q network weights.\n",
      "Step 5000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 411us/step - loss: 0.5799\n",
      "Step 5100: Updating target Q network weights with latest Q network weights.\n",
      "Step 5100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 432us/step - loss: 0.4297\n",
      "Step 5200: Updating target Q network weights with latest Q network weights.\n",
      "Step 5200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 452us/step - loss: 0.7171\n",
      "Step 5300: Updating target Q network weights with latest Q network weights.\n",
      "Step 5300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 450us/step - loss: 0.2723\n",
      "Step 5400: Updating target Q network weights with latest Q network weights.\n",
      "Step 5400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 490us/step - loss: 0.6681\n",
      "Step 5500: Updating target Q network weights with latest Q network weights.\n",
      "Step 5500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 397us/step - loss: 0.2806\n",
      "Step 5600: Updating target Q network weights with latest Q network weights.\n",
      "Step 5600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 409us/step - loss: 0.4105\n",
      "Step 5700: Updating target Q network weights with latest Q network weights.\n",
      "Step 5700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 463us/step - loss: 17.2579\n",
      "Step 5800: Updating target Q network weights with latest Q network weights.\n",
      "Step 5800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 416us/step - loss: 0.3813\n",
      "Step 5900: Updating target Q network weights with latest Q network weights.\n",
      "Step 5900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 514us/step - loss: 0.3977\n",
      "Step 6000: Updating target Q network weights with latest Q network weights.\n",
      "Step 6000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 631us/step - loss: 0.5276\n",
      "Step 6100: Updating target Q network weights with latest Q network weights.\n",
      "Step 6100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 438us/step - loss: 0.4122\n",
      "Step 6200: Updating target Q network weights with latest Q network weights.\n",
      "Step 6200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 412us/step - loss: 2.3807\n",
      "Step 6300: Updating target Q network weights with latest Q network weights.\n",
      "Step 6300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 535us/step - loss: 0.4317\n",
      "Step 6400: Updating target Q network weights with latest Q network weights.\n",
      "Step 6400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 480us/step - loss: 0.2993\n",
      "Step 6500: Updating target Q network weights with latest Q network weights.\n",
      "Step 6500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 435us/step - loss: 0.3363\n",
      "Step 6600: Updating target Q network weights with latest Q network weights.\n",
      "Step 6600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 557us/step - loss: 1.1509\n",
      "Step 6700: Updating target Q network weights with latest Q network weights.\n",
      "Step 6700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 431us/step - loss: 0.3753\n",
      "Step 6800: Updating target Q network weights with latest Q network weights.\n",
      "Step 6800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 426us/step - loss: 0.2360\n",
      "Step 6900: Updating target Q network weights with latest Q network weights.\n",
      "Step 6900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.3477\n",
      "Step 7000: Updating target Q network weights with latest Q network weights.\n",
      "Step 7000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 365us/step - loss: 0.8014\n",
      "Step 7100: Updating target Q network weights with latest Q network weights.\n",
      "Step 7100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 719us/step - loss: 0.3637\n",
      "Step 7200: Updating target Q network weights with latest Q network weights.\n",
      "Step 7200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 359us/step - loss: 60.9746\n",
      "Step 7300: Updating target Q network weights with latest Q network weights.\n",
      "Step 7300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 500us/step - loss: 0.8388\n",
      "Step 7400: Updating target Q network weights with latest Q network weights.\n",
      "Step 7400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 372us/step - loss: 0.8075\n",
      "Step 7500: Updating target Q network weights with latest Q network weights.\n",
      "Step 7500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 445us/step - loss: 0.2679\n",
      "Step 7600: Updating target Q network weights with latest Q network weights.\n",
      "Step 7600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 411us/step - loss: 0.4041\n",
      "Step 7700: Updating target Q network weights with latest Q network weights.\n",
      "Step 7700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 405us/step - loss: 2.1468\n",
      "Step 7800: Updating target Q network weights with latest Q network weights.\n",
      "Step 7800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 506us/step - loss: 0.8437\n",
      "Step 7900: Updating target Q network weights with latest Q network weights.\n",
      "Step 7900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 400us/step - loss: 0.3749\n",
      "Step 8000: Updating target Q network weights with latest Q network weights.\n",
      "Step 8000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 482us/step - loss: 2.2129\n",
      "Step 8100: Updating target Q network weights with latest Q network weights.\n",
      "Step 8100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 468us/step - loss: 0.3979\n",
      "Step 8200: Updating target Q network weights with latest Q network weights.\n",
      "Step 8200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 548us/step - loss: 0.2546\n",
      "Step 8300: Updating target Q network weights with latest Q network weights.\n",
      "Step 8300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 364us/step - loss: 0.3605\n",
      "Step 8400: Updating target Q network weights with latest Q network weights.\n",
      "Step 8400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 346us/step - loss: 0.4673\n",
      "Step 8500: Updating target Q network weights with latest Q network weights.\n",
      "Step 8500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 381us/step - loss: 0.3747\n",
      "Step 8600: Updating target Q network weights with latest Q network weights.\n",
      "Step 8600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 384us/step - loss: 0.2413\n",
      "Step 8700: Updating target Q network weights with latest Q network weights.\n",
      "Step 8700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 357us/step - loss: 4.6884\n",
      "Step 8800: Updating target Q network weights with latest Q network weights.\n",
      "Step 8800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 395us/step - loss: 0.4069\n",
      "Step 8900: Updating target Q network weights with latest Q network weights.\n",
      "Step 8900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 419us/step - loss: 0.3313\n",
      "Step 9000: Updating target Q network weights with latest Q network weights.\n",
      "Step 9000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 393us/step - loss: 0.3251\n",
      "Step 9100: Updating target Q network weights with latest Q network weights.\n",
      "Step 9100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 520us/step - loss: 0.3015\n"
     ]
    }
   ],
   "source": [
    "## Model training\n",
    "\n",
    "parameters = Parameters()\n",
    "parameters.state_length = list(states[0].shape)\n",
    "agent = Agent(parameters)\n",
    "env = Environment(states, steps=parameters.episode_length)\n",
    "summary = Summary()\n",
    "step = 0\n",
    "\n",
    "# Populate the replay memory with initial experiences\n",
    "agent.initialize_replay_memory(env, parameters, price_index=price_state_index)\n",
    "\n",
    "for e in range(parameters.episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    episode_learn_reward = 0\n",
    "    episode_actions = np.zeros(parameters.episode_length)\n",
    "    episode_actual_reward = 0\n",
    "    \n",
    "    for ii in range(parameters.episode_length):\n",
    "        \n",
    "        # Update the target network weights every X iterations\n",
    "        if step % parameters.update_target_weights_step_size == 0:\n",
    "            print('Step {}: Updating target Q network weights with latest Q network weights.'.format(step))\n",
    "            agent.target_network.interpolate_weights(agent.train_network.get_weights(), parameters.interpolation_factor)\n",
    "                \n",
    "        action = agent.act(state)\n",
    "        next_state, reward, index = env.step(action, price_index=price_state_index)\n",
    "        agent.replay_memory.add(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        episode_actions[ii] = action\n",
    "        episode_learn_reward += reward[action]\n",
    "        \n",
    "        if action > 0:\n",
    "            price = prices[index]\n",
    "            episode_actual_reward += price * -1.0025 if action == 1 else price * 0.9975\n",
    "        \n",
    "        if step % parameters.train_network_step_size == 0:\n",
    "            print('Step {}: Sampling replay memory and training Q network.'.format(step))\n",
    "            \n",
    "            agent.replay()\n",
    "            \n",
    "        step += 1\n",
    "    \n",
    "    summary.create(episode_actions, episode_learn_reward, episode_actual_reward, env.episode.get('start'), env.episode.get('current_index')+1, agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -3.4206614833386806\n",
      "Actual reward: -1.2288047923346763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a278b0438>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XlcVXX+x/HXh11QUQRXNlFUXFKT\nXDKzXHJpsZoWi0r7VTRN29RUWraXkzU17TUxtthkq9OklWZqZuaO+y6IgLiBC4jsy/f3x70WGAjK\n5Z4L9/N8PO6De77ne+55dzvw8WzfI8YYlFJKqZM8rA6glFLKtWhhUEopVYkWBqWUUpVoYVBKKVWJ\nFgallFKVaGFQSilViRYGpZRSlWhhUEopVYkWBqWUUpV4WR3gbAQHB5vIyEirYyilVIOydu3aw8aY\nkJr6NcjCEBkZSWJiotUxlFKqQRGRtNr000NJSimlKtHCoJRSqhItDEoppSrRwqCUUqoSLQxKKaUq\ncUhhEJEPRCRTRLZUM19E5A0RSRaRTSJyboV5E0Qkyf6a4Ig8Simlzp6j9hg+AkafZv4YINr+igfe\nBRCRIOApYADQH3hKRFo6KJNSSqmz4JD7GIwxv4hI5Gm6jAM+NrbniK4UkRYi0g64CFhgjDkKICIL\nsBWYzxyRSynlfLmFJew6dIKs3CIOnyjiaF4xAvh4eeDr5UFQU1+iggOIDA6gqW+DvJWq0XPW/5UO\nwN4K0xn2tura/0BE4rHtbRAeHl4/KZVSZ+xYXjELth9i5e4jbMzIJuVwHrV9lHy7QD8GRbVicOdg\nBncOpm2gX/2GVbXirMIgVbSZ07T/sdGYBCABIDY2tpabnVKqPhQUlzF7wz6+3bSflSlHKSs3BDf1\npU9YIOP6dKBH++a0ae5HSDNfWvr7IALFpeUUlZaTlVvEnsMn2HM4n637c/h5VxZfr98HQP/IIK6N\nDWVsr3YE6N6EZZz1zWcAYRWmQ4H99vaLTmn/2UmZlFJn6GBOITNWpPLpqnRyCkqICg7gzgujGNOz\nHT07NEekqn/r2Xh7ehDgC0EBPnRt2+y39vJyw/aDx/l5Zxb/XZvBw7M28fScrfypXyh/HtqJ9i2a\nOOG/TFUkprb7fDV9kO0cw3fGmJ5VzLsUuAcYi+1E8xvGmP72k89rgZNXKa0D+p0851Cd2NhYo2Ml\nKeU8R04U8caiJGauSqfcGC7p3pbbhnQkNqLlaYvBmTLGsDbtGJ+t3svsDfvwEOHa2FD+cnFnOmiB\nqDMRWWuMia2pn0P2GETkM2z/8g8WkQxsVxp5Axhj/gXMxVYUkoF84Fb7vKMi8hywxv5Rz9ZUFJRS\nzlNYUsaHy1J5Z3Ey+SVlXH9eGHcN7URYkH+9rE9EiI0MIjYyiAdGRvPOz7v5MnEvs9ZmcPfFnblz\naBS+Xp71sm71O4ftMTiT7jEoVf/Wph3l4a82kXI4jxExrZk8phudWzereUEHyziWzwvzdvD9pgN0\nDA7guXE9uSA62Ok5GoPa7jFoYVBKVVJYUsYrP+5k+q97aB/YhGl/6sWQ6BqH8K93v+zK4snZW0g9\nks8N/cN58rLuNPHRvYcz4dRDSUqpxiHpUC53zVxHcuYJbhwQzmNjY1zmXoMLu4Tww18v5NWFu3hv\nSQprUo/yxvi+dG/f3OpojY6OlaSUAuDbjfsZ9/YysvOL+c9t/fn7Vb1cpiic5OftyaNjYvjktgEc\nLyjhyreX8cnKWj17Rp0BLQxKubnSsnKe+XYr9362nph2zfnu3iEucejodC6IDmbe/UM4v3MrHv9m\nC098s4WSsnKrYzUaWhiUcmN5RaXc/nEiHy5LZeL5kXx2x8AGc/dxq6a+vD/hPO68MIr/rExj4oer\nyc4vtjpWo6CFQSk3lZlbyPUJK1iadJgXru7F01f0wMerYf1J8PQQHh0bwyvX9mbNnmNc/c5yMo7l\nWx2rwWtYW4FSyiFSsk5w9TvL2Z2Zx79v6ccN/Rv2+GN/6hfKp3cM4PCJIq791wqSM09YHalB08Kg\nlJtJOpTLde+tpKC4jC/uHMiwbm2sjuQQsZFBfHHnIErKDNe9t4LNGTlWR2qwtDAo5UZ2HsxlfMJK\nROCLOwdyTmgLqyM5VEy75nz150E08fbkhn+vZF36MasjNUhaGJRyE9sPHOeGf6/Ey1P4PH6gJXcx\nO0PH4ABm3TWI4KY+TPhgNVv26Z7DmdLCoJQb2J11grjpq/D18uCL+EF0CmlqdaR61S6wCTPvGEhz\nP29uen8VOw4etzpSg6KFQalGbn92ATdPX4WHwKd3DCQyOMDqSE7RoUUTPr1jAH5entw0fZWekD4D\nWhiUasSO5hVz8/uryC0s5aNb+9PRTYrCSRGtAph5xwAAJnywmszjhRYnahi0MCjVSOUXl3Lrh6vJ\nOFbA9Amx9OwQaHUkS3QKacqHE/tzLL+YWz9aw4miUqsjuTwtDEo1QmXlhvs/38DmfTm8deO5DIhq\nZXUkS/UKDeTtuHPZcTCXv8xcp8Nn1MAhhUFERovIThFJFpHJVcx/VUQ22F+7RCS7wryyCvPmOCKP\nUu7uxR92sGDbIZ68rDsjuzeO+xTq6uKurXnhql78siuLx77eTEN85ICz1HnoRBHxBN4GRmJ7hvMa\nEZljjNl2so8x5oEK/e8F+lb4iAJjTJ+65lBK2Xy2Op2EX1K4ZVAEEwd3tDqOS7nuvDAysgt4Y1ES\nXds24/YhUVZHckmO2GPoDyQbY1KMMcXA58C40/S/AfjMAetVSp1iefJhnvhmC0O7hPDkZd2tjuOS\n/jo8mjE92/L3udtZsivL6jguyRGFoQOwt8J0hr3tD0QkAugI/FSh2U9EEkVkpYhc6YA8SrmljGP5\n3P3pOjoGB/DWjX3x8tRTiFXx8BBevrY3Xdo0495P15GSpZexnsoRW45U0VbdwbvxwCxjTFmFtnD7\no+ZuBF4TkU5VrkQk3l5AErOytMorVVFhSRl3fbKO0jLDezf3o5mft9WRXFqArxf/viUWL08Pbv84\nkdzCEqsjuRRHFIYMIKzCdCiwv5q+4znlMJIxZr/9ZwrwM5XPP1Tsl2CMiTXGxIaEuPZDRJRyJmMM\nT3yzhc37cvjn9X2IauR3NTtKWJA/78SdS9qRfCb9d5OejK7AEYVhDRAtIh1FxAfbH/8/XF0kIl2B\nlsCKCm0tRcTX/j4YGAxsO3VZpVT1Pl2dzldrM7hvWGe9AukMDYxqxcOjujJ380E+Wp5qdRyXUefC\nYIwpBe4B5gPbgS+NMVtF5FkRuaJC1xuAz03lshwDJIrIRmAxMK3i1UxKqdPbuj+HZ+ZsY2iXEO4f\n0cXqOA1S/JAoRsS04e9zt+torHbSEHefYmNjTWJiotUxlLJUXlEpl7/5K3nFpcy7/0KCAnysjtRg\n5eSXcOmbSykvN3x335BG+12KyFr7Od3T0ssWlGqgnvhmC6lH8nh9fN9G+4fMWQL9vXk3rh+HTxTz\nyKyNbn++QQuDUg3QrLUZfL1+H/cNj2agmw934Si9QgOZPKYbC7dn8snKNKvjWEoLg1INTErWCZ74\nZgsDo4K4d1i01XEalVsHR3JR1xCe/347Ow/mWh3HMloYlGpASsrKeeDLjfh4efDa9X3x9KjqNiJ1\ntkSEf1zTm2Z+Xtz32XoKS8pqXqgR0sKglKubORMiI8HDg7fG3cPGvdlMvaonbQP9rE7WKIU08+Xl\na3uz81Au0+btsDqOJbQwKOXKZs6E+HhIS2Nduy681X00V+34hcs2L7Y6WaN2UdfWTDw/ko+Wp7Is\n+bDVcZxOC4NSrmzKFMjPJ8/bjwcv/Rttc4/wzLy3bO2qXk0a3Y2o4AAembWJ4242ZIYWBqVcWXo6\nANMumkhay7a8/P2rNC/O/61d1Z8mPp68cl1vDuQU8Ny37nXfrRYGpVxZeDjLw3vxn3MvY+Labxm0\nd/Nv7ar+9Q1vyV0XdeKrtRks3HbI6jhOo4VBKReW9+xUJo39KxHH9vPIko9tjf7+MHWqtcHcyH3D\no+nWthmTv97Msbxiq+M4hRYGpVzYS4G9yQhszT/WfU6TsmKIiICEBIiLszqa2/D18uSf1/UhO7+Y\n5753j0NKWhiUclErU44wY0UaE87vSP81i6C8HFJTtShYoHv75vx5aCe+XrePn3dmWh2n3mlhUMoF\nFZaUMem/mwgP8ueR0V2tjqOAe4d3plNIAFP+t4UTRaVWx6lXWhiUckGvL0oi7Ug+067uhb+Pl9Vx\nFLZDSi9d05v9OQW89EPjvvFNC4NSLmbb/uMk/JLCtf1COb9zsNVxVAX9Iloy8fxIPl6RxprUo1bH\nqTcOKQwiMlpEdopIsohMrmL+RBHJEpEN9tftFeZNEJEk+2uCI/Io1VCVlRse/XoTLf29mXJpjNVx\nVBUeuqQrHVo04bGvN1NcWm51nHpR58IgIp7A28AYoDtwg4h0r6LrF8aYPvbXdPuyQcBTwACgP/CU\niLSsayalGqqPlqeyMSOHJy/vQQt/fcaCKwrw9eLZcT1IyjzBv5emWB2nXjhij6E/kGyMSTHGFAOf\nA+NquewoYIEx5qgx5hiwABjtgExKNTj7swt45cedXNw1hMvPaWd1HHUaw2PaMLZXW95YlETakTyr\n4zicIwpDB2BvhekMe9up/iQim0RkloiEneGySjV6z3y7lXJjeHZcT0R0OG1X99TlPfD29ODxb7Y0\nuie+OaIwVLUFn/otfQtEGmPOARYCM85gWVtHkXgRSRSRxKysrLMOq5Qr+mnHIeZvPcS9w6IJC/K3\nOo6qhTbN/Xh4VFeWJh1mzsb9VsdxKEcUhgwgrMJ0KFDpWzLGHDHGFNkn/w30q+2yFT4jwRgTa4yJ\nDQkJcUBspVxDQXEZT83ZSufWTbljSJTVcdQZuGlgBL1DA3n+++2NagRWRxSGNUC0iHQUER9gPDCn\nYgcRqXjA9Apgu/39fOASEWlpP+l8ib1NKbfx9uJk9h4t4LlxPfHx0ivIGxJPD+G5K3ty+EQRry1I\nsjqOw9R5KzTGlAL3YPuDvh340hizVUSeFZEr7N3uE5GtIrIRuA+YaF/2KPActuKyBnjW3qaUW9id\ndYL3ftnN1X07MKhTK6vjqLNwTmgLbuwfzowVqWw/cNzqOA4hDfGkSWxsrElMTLQ6hlJ1Yozhlg9W\ns2FvNj/97SJCmvlaHUmdpez8Yoa9soSo4AC++vMgl714QETWGmNia+qn+61KWeTHbYdYmnSYB0d2\n0aLQwLXw92Hy6G4kph3j63X7rI5TZ1oYlLJAYUkZz367ja5tmnHzwAir4ygHuKZfKH3DW/DCvO3k\nFDTsE9FaGJSywLs/72ZfdgHPjOuBl6f+GjYGHh7Cc+N6ciSvmDcWNewT0bpFKuVk6UfyeXfJbq7o\n3Z6BUXrCuTHp2SGQ8eeFMWN5KsmZuVbHOWtaGJRysqlzt+HlITw2VgfJa4weuqQrTXw8eebbbQ32\njmgtDEo50bLkw8zfeoi7L+5M20A/q+OoetCqqS8PjOjC0qTDLNzeMJ/2poVBKScpLSvn2W+3ERbU\nhNsu6Gh1HFWPbh4UQXTrpjz33TYKS8qsjnPGtDAo5SSfrdnLzkO5TBkbg5+3p9VxVD3y9vTgqct7\nkH40n/d/3WN1nDOmhUEpJ8jJL+GfP+5kUFQrRvVoa3Uc5QQXRAczIqYN7yxOJjO30Oo4Z0QLg1JO\n8NqiXeQUlPDk5d1d9q5Y5XhTLo2huKycV+bvsjrKGdHCoFQ9S848wccr0hjfP5yYds2tjqOcqGNw\nABMGRfLl2r1s3Z9jdZxa08KgVD2bNm87/t6ePDiyi9VRlAXuHR5NiybePNuALl/VwqBUPVqWbLtk\n8e5hnQluquMhuaPAJt48eElXVu05yvytB62OUytaGJSqJ2Xlhue+20ZoyyZMPD/S6jjKQjecF0aX\nNk15Yd4OikvLrY5TIy0MStWTWWv3suNgLo+O0ctT3Z2XpwePjY0h7Ug+/1mZZnWcGmlhUKoenCgq\n5R/zd9EvoiVje+nlqQqGdglhSHQwbyxKIju/2Oo4p+WQwiAio0Vkp4gki8jkKuY/KCLbRGSTiCwS\nkYgK88pEZIP9NefUZZVqiBKW7ObwiSIevzRGL09VAIjYxsc6XljCmz8lWx3ntOpcGETEE3gbGAN0\nB24Qke6ndFsPxBpjzgFmAS9VmFdgjOljf12BUg3cwZxCEpamcHnv9vQNb2l1HOVCYto157p+YXy8\nIpW0I3lWx6mWI/YY+gPJxpgUY0wx8DkwrmIHY8xiY0y+fXIlEOqA9Srlkl75cSfl5fDIqK5WR1Eu\n6G+XdMHLw4MXf9hhdZRqOaIwdAD2VpjOsLdV5zZgXoVpPxFJFJGVInJldQuJSLy9X2JWVlbdEitV\nT7YfOM6sdRlMOD+CsCB/q+MoF9S6uR93Do1i7uaDrE07ZnWcKjmiMFR1ALXKuzhE5CYgFvhHheZw\n+8OpbwReE5FOVS1rjEkwxsQaY2JDQkLqmlmpevH3udtp7ufNPRdHWx1FubA7hkQR0syXv8/d7pI3\nvTmiMGQAYRWmQ4H9p3YSkRHAFOAKY0zRyXZjzH77zxTgZ6CvAzIp5XRLdmWxNOkw9w7rTKC/t9Vx\nlAsL8PXiwZFdWJt2zCVvenNEYVgDRItIRxHxAcYDla4uEpG+wHvYikJmhfaWIuJrfx8MDAa2OSCT\nUk5VVm54Ye52woKacPOgiJoXUG7v2n6hdG7dlBd/2ElJmWvd9FbnwmCMKQXuAeYD24EvjTFbReRZ\nETl5ldE/gKbAV6dclhoDJIrIRmAxMM0Yo4VBNTjfrN/HjoO5PDyqG75eejObqpmXpwePjunGnsN5\nfLY63eo4lXg54kOMMXOBuae0PVnh/YhqllsO9HJEBqWsUlhSxis/7uSc0EAu69XO6jiqARnWrTUD\no4J4fWESV/XtQDM/1zgEqXc+K1VHHy1PZX9OIZPHdMPDQ29mU7V38qa3I3nFJPySYnWc32hhUKoO\njuUV8/biZIZ1a835nYKtjqMaoHNCW3DZOe2YvnQPmcdd40lvWhiUqoO3FyeTV1TKpNHdrI6iGrCH\nR3WlpKyc1xYlWR0F0MKg1FnLOJbPxyvS+NO5oXRt28zqOKoBi2gVQNyAcL5Ys5fdWSesjqOFQamz\n9c8FuxCBB/TJbMoB7h0ejZ+XBy+5wFAZWhiUOgvbDxznf+v3MXFwJO1bNLE6jmoEgpv6cufQTszf\neoi1aUctzaKFQamz8NIPO2jm68Vfhna2OopqRG4f0pGQZr5Mm7fD0qEytDAodYZW7D7C4p1Z3H2x\nDn2hHMvfx4v7h0ezJvUYP+3IrHmBeqKFQakzYIxh2g87aBfoxwR9jrOqB9efF0bH4ABe/GEHZeXW\n7DVoYVDqDMzbcpCNe7N5YEQXfY6zqhfenh48dElXdh06wdfrMizJoIVBqVoqLSvn5fk76dKmKX/q\np8+aUvVnbK+29A4N5NUFuygsKXP6+rUwKFVLXyZmkHI4j4dHdcNTh75Q9UhEmDSmG/tzCvnPijSn\nr18Lg1K1UFBcxmsLdxEb0ZIRMa2tjqPcwPmdghnaJYS3FieTU1Di1HVrYVCqFj5YtofM3CImj+mG\niO4tKOd4ZHRXcgpKeG/JbqeuVwuDUjU4llfMv5bsZkRMa2Ijg6yOo9xIj/aBjOvTng+W7eGQEwfY\nc0hhEJHRIrJTRJJFZHIV831F5Av7/FUiEllh3qP29p0iMsoReZRypHd+TuZEUSkPj9KB8pTz/W1k\nV8rKDa87cYC9OhcGEfEE3gbGAN2BG0Sk+yndbgOOGWM6A68CL9qX7Y7tUaA9gNHAO/bPU8ol7Msu\nYIYOlKcsFN7Knxv72wbYS3HSAHuO2GPoDyQbY1KMMcXA58C4U/qMA2bY388ChovtQO044HNjTJEx\nZg+QbP88pVzCawt2ATpQnrLWyQH2Xv5xp1PW54jC0AHYW2E6w95WZR/7M6JzgFa1XFYpSyQdyuW/\n6zK4ZWAEHXSgPGWh4Ka+3D4kirmbD7I5I6fe1+eIZz5XdYnGqfdxV9enNsvaPkAkHogHCA8PP5N8\nSp2Vl+bvJMDHi7sv1oHylPXuuDCKdoF+dGtX/4c0HbHHkAGEVZgOBfZX10dEvIBA4GgtlwXAGJNg\njIk1xsSGhIQ4ILZS1VubdpQF2w5x59AoWgb4WB1HKZr6ejG+fzjenvV/Makj1rAGiBaRjiLig+1k\n8pxT+swBJtjfXwP8ZGxjys4BxtuvWuoIRAOrHZBJqbNmjOHFeTsJaebL/13Q0eo4SjldnQ8lGWNK\nReQeYD7gCXxgjNkqIs8CicaYOcD7wH9EJBnbnsJ4+7JbReRLYBtQCtxtjHH+wCBKVfDTjkxWpx7l\nuSt74u/jiKOtSjUsYuXDIM5WbGysSUxMtDqGaoTKyg1jX19KUWkZCx4c6pTddqWcRUTWGmNia+qn\nW71SFXyzfh87D+Xy0KiuWhSU29ItXym7wpIy/rlgF706BDK2Zzur4yhlGS0MStl9sjKNfdkFTB7T\nDQ8dVlu5MS0MSgHHC0t4e3EyQ6KDGdw52Oo4SllKC4NSQMKSFI7llzBptA6Up5QWBuX2Dh0vZPqv\nKVzRuz09OwRaHUcpy2lhUG7vtYVJlJUbHrqkq9VRlHIJWhiUW0vOPMGXiXuJGxBBeCt/q+Mo5RK0\nMCi39o/5O2ji7cm9w3SgPKVO0sKg3NbatGPM33qI+AujaNXU1+o4SrkMLQzKLRljmDZvO8FNfblN\nB8pTqhItDMotLdh2iDWpx3hgZDQBvjpQnlIVaWFQbqe0rJwXf9hBVEgA18eG1byAUm5GC4NyO18m\nZrA7K49Jo7vhpQPlKfUH+luh3Ep+cSmvLtxFbERLLunexuo4SrmkOhUGEQkSkQUikmT/2bKKPn1E\nZIWIbBWRTSJyfYV5H4nIHhHZYH/1qUsepWoyfekesnKLeHRsDCI6UJ5SVanrHsNkYJExJhpYZJ8+\nVT5wizGmBzAaeE1EWlSY/7Axpo/9taGOeZSqVmZuIf9aspvRPdrSL+IP/4ZRStnVtTCMA2bY388A\nrjy1gzFmlzEmyf5+P5AJhNRxvUqdsdcWJlFcWs6kMTpQnlKnU9fC0MYYcwDA/rP16TqLSH/AB9hd\noXmq/RDTqyKidxmpepF0KJcv1uzlpoERdAwOsDqOUi6txsIgIgtFZEsVr3FnsiIRaQf8B7jVGFNu\nb34U6AacBwQBk06zfLyIJIpIYlZW1pmsWrmxmZtnEvlaJBe8/jJlJp/27TZaHUkpl1fjnT3GmBHV\nzRORQyLSzhhzwP6HP7Oafs2B74HHjTErK3z2AfvbIhH5EHjoNDkSgASA2NhYU1NupWZunkn8t/GU\nFXaibXl/jnl9yAML5hHgZ4jrFWd1PKVcVl0PJc0BJtjfTwBmn9pBRHyA/wEfG2O+OmVeO/tPwXZ+\nYksd8yj1mymLppBfXEDLktsolUxyvb4lvySfKYumWB1NKZdW18IwDRgpIknASPs0IhIrItPtfa4D\nLgQmVnFZ6kwR2QxsBoKB5+uYR6nfpOekE1B2Mb6mM9leH2Ok+Ld2pVT16jRIjDHmCDC8ivZE4Hb7\n+0+AT6pZflhd1q/U6YQ360xp5i0UyS7yPJf83h4YbmEqpVyf3vmsGq1hbf6OF8Ec854OYjst5e/t\nz9ThUy1OppRr08KgGpeZMyEykoPNg1mx3oOe/gdoG5SHIEQERpBweYKeeFaqBjresGo8Zs6E+HjI\nz+flsfdThgfvvDuV8Jenwl+1GChVW7rHoBqPKVMgP58tbTrx357DuXXtHMIPptralVK1poVBNR7p\n6RjgmeHxBOUf5+7lX/zWrpSqPT2UpBqP8HC+axLOmrAevPDDmzQvzv+tXSlVe1oYVKNR8NxUpq0o\nofuh3Vy3aYGt0d8fpupVSEqdCT2UpBqNhHb92dc8hKe2zMETAxERkJAAcXriWakzoXsMqlHYn13A\nu0uSubRXOwZMW2h1HKUaNN1jUI3CtHk7MAYm67MWlKozLQyqwVuZcoQ5G/dz59BOhAX5Wx1HqQbP\nrQpD2pE8VqYcsTqGcqDSsnKenrOVDi2acNfQTlbHUapRcKvC8NBXG7nvs/WcKCq1OopykE9WprHj\nYC5PXNadJj6eVsdRqlFwq8Iw5dLuZJ0o4vWFu6yOohzg8IkiXlmwiyHRwYzq0cbqOEo1Gm5VGPqE\ntWD8eeF8sCyVnQdzrY6j6uilH3ZQUFzGU5f3wPasJ6WUI7hVYQB4ZFRXmvl58cTsLRijTwhtqBJT\nj/JlYga3XdCRzq2bWh1HqUalToVBRIJEZIGIJNl/tqymX1mFp7fNqdDeUURW2Zf/wv4Y0HrVMsCH\nSaO7sXrPUWZv2F/fq1P1oLSsnMe/2UL7QD/uGx5tdRylGp267jFMBhYZY6KBRfbpqhQYY/rYX1dU\naH8ReNW+/DHgtjrmqZXrY8PoHdaC57/fTk5BiTNWqRzoo+Wp7DiYy5OX9yDAV+/RVMrR6loYxgEz\n7O9nAFfWdkGxHRQeBsw6m+XrwsNDeH5cT47mFfHy/J3OWKVykAM5Bby6YBfDurXWE85K1ZO6FoY2\nxpgDAPafravp5yciiSKyUkRO/vFvBWQbY05eO5oBdKhuRSISb/+MxKysrDrGhl6hgdwyKJJPVqWx\nLv1YnT9POcez326jzBieuUJPOCtVX2osDCKyUES2VPEadwbrCTfGxAI3Aq+JSCegqt/qas8GG2MS\njDGxxpjYkJCQM1h19f52SRfaNPPjsa83U1JW7pDPVPVn8Y5M5m05yL3DovUOZ6XqUY2FwRgzwhjT\ns4rXbOCQiLQDsP/MrOYz9tt/pgA/A32Bw0ALETl5kDgUcOrZ4GZ+3jx9RQ92HMzlw2V7nLlqdYby\nikp5/JstdGnTlDuGRFkdR6lGra6HkuYAE+zvJwCzT+0gIi1FxNf+PhgYDGwztmtFFwPXnG75+jaq\nRxtGxLTh1QVJ7D2a7+zVq1p6+ced7M8p4IWrz8HHy+2uslbKqer6GzYNGCkiScBI+zQiEisi0+19\nYoBEEdmIrRBMM8Zss8+bBDwoIsnYzjm8X8c8Z0xEeGZcD0Rgyjd6b4Mr2rA3m4+Wp3LTgAj6RVR5\nRbRSyoGkIf4hjI2NNYmJiQ6pZuexAAAPzElEQVT9zBnLU3lqzlZevrY31/QLdehnq7NXUlbO5W/+\nSnZ+CQsevJBmft5WR1KqwRKRtfbzvael++R2Nw+MIDaiJc99t43M3EKr4yi7hF9S2HEwl2fH9dCi\noJSTaGGw8/AQXrzmHApKynhq9lar4yhg16FcXl+YxNhebbmkR1ur4yjlNrQwVNAppCn3D49m3paD\nzNt8wOo4bq20rJyHvtpIUz8vnh3X0+o4SrkVLQyniL8wih7tm/PE7C0cOVFkdRy3lbA0hU0ZOTw3\nrifBTX2tjqOUW9HCcApvTw9eua43OQUlPK5XKVki6VAury2wHUK69Jx2VsdRyu1oYahCt7bNeWBk\nF+ZtOUjHadfi8YwHka9FMnPzTKujNXoleghJKctpYahGs5arKPHcRVnOtXiYlqTlpBH/bbwWh3r2\n5k/JbMzI4fkr9RCSUlbRwlCNJxZPIdPrZQQvWhXfBwbyS/KZsmiK1dEarXXpx3h7cTJXn9uBsb30\nEJJSVtHCUI30nHRKPfaT7f0RTcpjaVp26W/tyvHyikp54IsNtG3ux9NX9LA6jlJuTQtDNcIDwwHI\n9fyOAo9Egkpuw7s84rd25VjPf7+d9KP5/PO63jTXG9mUspQWhmpMHT4Vf29/EDjs8yrl5NG6ZBJP\nD51qdbRG54ctB/lsdTrxF0YxIKqV1XGUcntaGKoR1yuOhMsTiAiMwMhxPFt+ild5OHvSe1sdrVHZ\nezSfR2Zt5JzQQP42sqvVcZRSgD4w9zTiesUR1yvut+nnvtvG+7/uYXDnYB2iwQFKysq57/P1GANv\n3XCuDqetlIvQ38Qz8MjorvTqEMhDX20k/Yg+u6GuXv5xJ+vTs3nhT70Ib6VPZFPKVWhhOAO+Xp68\nE3cuAH95cwGFUZ3BwwMiI2Gm3t9wJhbvyOS9JSnEDQjnsnPaWx1HKVVBnQqDiASJyAIRSbL//MNT\nVETkYhHZUOFVKCJX2ud9JCJ7KszrU5c8zhAW5M8rbbLZUujFc9GjwBhIS4P4eC0OtZR2JI/7P19P\nTLvmPHFZd6vjKKVOUdc9hsnAImNMNLDIPl2JMWaxMaaPMaYPMAzIB36s0OXhk/ONMRvqmMcpRr44\niTtXzmJm37H8r/tFtsb8fJiiN7/VpKC4jDv/sxYR4b2b+uHn7Wl1JKXUKepaGMYBM+zvZwBX1tD/\nGmCeMaZhH6BPT+fhXz5mQPpmJo++l01tO//WrqpnjGHy15vYeSiX18f30fMKSrmouhaGNsaYAwD2\nn61r6D8e+OyUtqkisklEXhWRhjE4Tng4Xqacd755geD8bG667nEi/9oCjyeNDrZ3Gh8uS2X2hv38\nbWQXLupa06ailLJKjYVBRBaKyJYqXuPOZEUi0g7oBcyv0Pwo0A04DwgCJp1m+XgRSRSRxKysrDNZ\nteNNnQr+/rQqOM5Vm54np0lTCptMweClg+1V4+edmTz//TYu6d6Gv1zU2eo4SqnTqLEwGGNGGGN6\nVvGaDRyy/8E/+Yc/8zQfdR3wP2NMSYXPPmBsioAPgf6nyZFgjIk1xsSGhITU9r+vfsTFQUICRETw\nVv89HPZ5Fb/yGFqV3K2D7VVh58Fc7vl0Pd3aNufV6/vg4SFWR1JKnUZdDyXNASbY308AZp+m7w2c\nchipQlERbOcnttQxj/PExUFqKukthHzPZWR7fUrTspEEll4P6GB7J2XlFvF/H60hwNeT9yfGEuCr\n91Qq5erqWhimASNFJAkYaZ9GRGJFZPrJTiISCYQBS05ZfqaIbAY2A8HA83XM43QnB9XL8fqUE56L\naFF6MwGlw3WwPWxXIN3xcSJH84qZfst5tAtsYnUkpVQt1Omfb8aYI8DwKtoTgdsrTKcCHaroN6wu\n63cFU4dPJf7bePJL8jni/SaeJohWJfcyMabQ6miWKi4t566Za9mUkc27N/WjV2ig1ZGUUrWkdz7X\nUcXB9kTKaBL8Ce1aCF8tC2Tj3myr41mivNzw0Fcb+XlnFlOv6sUoHVdKqQZFGuLD7mNjY01iYqLV\nMap1MKeQa99bzvGCUj67YyDd2ze3OpLTGGN4es5WZqxIY9Lobtx1USerIyml7ERkrTEmtqZ+usdQ\nD9oG+vHp7QMJ8PHkpvdXkXQo1+pITmGM4cUfdjJjRRrxF0bx56FRVkdSSp0FLQz1JCzIn5l3DMTL\nQ7hx+ip2Z52wOlK9Msbw97nb+deS3cQNCOfRMd2wXWymlGpotDDUo47BAXx6xwDKyw3Xv7eSbfuP\nWx2pXhhjePa7bfx76R4mnh/J81f21KKgVAOmhaGedW7djC/uHIS3pzA+YQVr045aHcmhysoNU77Z\nwofLUvm/wR156vLuWhSUauC0MDhB59ZN+erPg2jV1Jebpq9myS6Lh/RwkJMjpX66Kp27LurEE5fF\naFFQqhHQwuAkoS39+fLOQUQGB3DbR2uYuSrN6kh1cuREETf8eyWLdhzimSt6MGm0nlNQqrHQwuBE\nIc18+fLOgQyJDmbK/7bw1OwtlJaVWx3rjO04eJyr313O9gPHeTeuHxPOj7Q6klLKgbQwOFkzP2+m\nTziPO4Z0ZMaKNCZ+uIYjJ4qsjlVr/1ufwZVvL6OguIxP7xjI6J5685pSjY0WBgt4eghTLu3OS9ec\nw+rUo4x+fSlLk1z7vENhSRmPf7OZB77YSO/QFnx33wX0i/jDk1yVUo2AFgYLXRcbxuy7BxPYxJub\n31/N3+dup7jU9Q4tbdibzWVv/sonK9O588IoZt4+gNbN/KyOpZSqJ1oYLBbTrjnf3nMBcQPCSfgl\nhbFvLGXF7iNWxwJsewnT5u3g6neWkVdUyoz/68+jY2Pw8tTNRqnGTMdKciGLd2TyxOwtZBwr4Kq+\nHXhsbAwhzZz/tFNjDN9vPsCLP+xg79ECxp8XxmOXxtDcz9vpWZRSjlPbsZL0qSku5OJurVkQNZR3\nfk7mX0t2M3/rQW4eFEH8kChaNa3/AmGMYU3qMV6Yt5316dl0a9uMmbcPYHDn4Hpft1LKddRpj0FE\nrgWeBmKA/vbnMFTVbzTwOuAJTDfGnHygT0fgc2zPe14H3GyMKa5pvY11j6GilKwTvLEoiTkb9+Pr\n5clNA8OJGxBBZHCAw9dVUlbO3M0H+GBZKhv3ZtO6mS8PjerKn84NxVMfw6lUo1HbPYa6FoYYoBx4\nD3ioqsIgIp7ALmxPeMsA1gA3GGO2iciXwNfGmM9F5F/ARmPMuzWt1x0Kw0m7s07w1k/JzN6wj3ID\n/TsGcW2/UEbEtKFlgE/1C86cCVOmQHo6M4cGMWUEpJceJTwwnKnDp3JDjxtZv/cYczcf5PtNBzh4\nvJCOwQHcOjiSa/qF4u+jO5NKNTZOKQwVVvYz1ReGQcDTxphR9ulH7bOmAVlAW2NM6an9TsedCsNJ\nh44X8t91GXyVmMGew3mIQM/2gQzuHEzf8BZEBQcQ3sofXy9PW1GIj4f8fGb2gvjLoMCnCd7lEfia\nLvgTQ4jXYHILPPDx9OCC6GDiBoRzcdfWeOgeglKNliudY+gA7K0wnQEMAFoB2caY0grtf3j8p7Jp\n09yPv1zUmbuGdmLD3myWJh3m1+TDvP9rCiVltuLuIdC6mR9++4rxHf8SXmWlbG3TjFblgXgU/n55\naSlHyPXYwOvj/49h3VrTTE8qK6UqqLEwiMhCoKrbW6cYY2bXYh1V/RPUnKa9uhzxQDxAeHh4LVbb\nOIkIfcNb0je8JfcNjyavqJSkzBOkHs4j5XAeB7ILKFo5nyJPb0o9vVjdPp1Sj2OUk0OJxz6KPZIo\nk6MIwrg+T1j9n6OUckE1FgZjzIg6riMDCKswHQrsBw4DLUTEy77XcLK9uhwJQALYDiXVMVOjEeDr\nRZ+wFvQJa/F748PjIM02SF/kXyGtxR+XCw903+KqlDo9Z9yptAaIFpGOIuIDjAfmGNvJjcXANfZ+\nE4Da7IGomkydCv7+treLwP+U67z8vf2ZOnyqBcGUUg1BnQqDiFwlIhnAIOB7EZlvb28vInMB7HsD\n9wDzge3Al8aYrfaPmAQ8KCLJ2M45vF+XPMouLg4SEiAigrgtQsLyVkR4tUIQIgIjSLg8gbhecVan\nVEq5KL3zWSml3ERtr0rSQW+UUkpVooVBKaVUJVoYlFJKVaKFQSmlVCVaGJRSSlWihUEppVQlWhiU\nUkpVooVBKaVUJQ3yBjcRyQLSznLxYGzjNLk7/R5s9Hv4nX4XNo35e4gwxoTU1KlBFoa6EJHE2tz5\n19jp92Cj38Pv9Luw0e9BDyUppZQ6hRYGpZRSlbhjYUiwOoCL0O/BRr+H3+l3YeP234PbnWNQSil1\neu64x6CUUuo03KowiMhoEdkpIskiMtnqPM4iImEislhEtovIVhG5394eJCILRCTJ/rOl1VmdQUQ8\nRWS9iHxnn+4oIqvs38MX9icNNmoi0kJEZonIDvt2McgdtwcRecD+O7FFRD4TET933B5O5TaFQUQ8\ngbeBMUB34AYR6W5tKqcpBf5mjIkBBgJ32//bJwOLjDHRwCL7tDu4H9vTBE96EXjV/j0cA26zJJVz\nvQ78YIzpBvTG9n241fYgIh2A+4BYY0xPwBPbo4fdcXuoxG0KA9AfSDbGpBhjioHPgXEWZ3IKY8wB\nY8w6+/tcbH8EOmD7759h7zYDuNKahM4jIqHApcB0+7QAw4BZ9i6N/nsQkebAhdgfpWuMKTbGZOOG\n2wPgBTQRES/AHziAm20PVXGnwtAB2FthOsPe5lZEJBLoC6wC2hhjDoCteACtrUvmNK8BjwDl9ulW\nQLb92eTgHttFFJAFfGg/pDZdRAJws+3BGLMPeBlIx1YQcoC1uN/28AfuVBikija3uiRLRJoC/wX+\naow5bnUeZxORy4BMY8zais1VdG3s24UXcC7wrjGmL5BHIz9sVBX7OZRxQEegPRCA7VDzqRr79vAH\n7lQYMoCwCtOhwH6LsjidiHhjKwozjTFf25sPiUg7+/x2QKZV+ZxkMHCFiKRiO5Q4DNseRAv7oQRw\nj+0iA8gwxqyyT8/CVijcbXsYAewxxmQZY0qAr4Hzcb/t4Q/cqTCsAaLtVxz4YDvJNMfiTE5hP47+\nPrDdGPPPCrPmABPs7ycAs52dzZmMMY8aY0KNMZHY/v//ZIyJAxYD19i7ucP3cBDYKyJd7U3DgW24\n2faA7RDSQBHxt/+OnPwe3Gp7qIpb3eAmImOx/QvRE/jAGDPV4khOISIXAEuBzfx+bP0xbOcZvgTC\nsf2SXGuMOWpJSCcTkYuAh4wxl4lIFLY9iCBgPXCTMabIynz1TUT6YDsB7wOkALdi+4eiW20PIvIM\ncD22K/fWA7djO6fgVtvDqdyqMCillKqZOx1KUkopVQtaGJRSSlWihUEppVQlWhiUUkpVooVBKaVU\nJVoYlFJKVaKFQSmlVCVaGJRSSlXy/xUnMqXkT4FBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a27900c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine episode\n",
    "\n",
    "episode_num = 97\n",
    "\n",
    "episode_summary = summary.summarize_episode(episode_num)\n",
    "episode_price = summary.get_price(prices, episode_num)\n",
    "actions = episode_summary.get('actions')\n",
    "\n",
    "print('Training reward: {}'.format(episode_summary.get('train_reward')))\n",
    "print('Actual reward: {}'.format(episode_summary.get('actual_reward')))\n",
    "\n",
    "# Plot graph of buy/sell actions\n",
    "buy_indices = [x for x in range(len(actions)) if actions[x] == 1]\n",
    "buy_prices = [episode_price[x] for x in buy_indices]\n",
    "\n",
    "sell_indices = [x for x in range(len(actions)) if actions[x] == 2]\n",
    "sell_prices = [episode_price[x] for x in sell_indices]\n",
    "\n",
    "plt.plot(episode_price)\n",
    "plt.scatter(buy_indices, buy_prices, color='r')\n",
    "plt.scatter(sell_indices, sell_prices, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.replay_memory.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, actual, epsilon = summary.summarize_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
