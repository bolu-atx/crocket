{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/bhsu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "from enum import Enum\n",
    "from itertools import cycle\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange, seed\n",
    "from os.path import join\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = '/home/b3arjuden/crocket/sql_data/PRODUCTION40'\n",
    "\n",
    "path = '/Users/bhsu/crypto/sql_data/PRODUCTION40'\n",
    "\n",
    "file = 'BTC-ETH.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = join(path, file)\n",
    "\n",
    "data = pd.read_csv(file_path, \n",
    "                   dtype={'time': str, 'buy_order': int, 'sell_order': int},\n",
    "                   converters={'price': Decimal,\n",
    "                               'wprice': Decimal,\n",
    "                               'base_volume': Decimal,\n",
    "                               'buy_volume': Decimal,\n",
    "                               'sell_volume': Decimal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_transform(df, n):\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    \n",
    "    nrows = n * floor(df.shape[0] / n)\n",
    "    df = df.iloc[:nrows, :]\n",
    "    \n",
    "    df1['time'] = df.loc[df.index[::n], 'time'].reset_index(drop=True)\n",
    "    df1['open'] = (df.loc[::n, 'wprice'].values)\n",
    "    df1['high'] = (df.loc[:, 'wprice'].groupby(df.index // n).max()) # TODO: estimate using 2 standard deviation from mean\n",
    "    df1['low'] = (df.loc[:, 'wprice'].groupby(df.index // n).min()) # TODO: estimate using 2 standard deciation from mean \n",
    "    df1['close'] = (df.loc[(n-1)::n, 'wprice'].values)\n",
    "    \n",
    "    df1['buy_volume'] = df.loc[:, 'buy_volume'].groupby(df.index // n).sum()\n",
    "    df1['sell_volume'] = df.loc[:, 'sell_volume'].groupby(df.index // n).sum()\n",
    "    df1['buy_order'] = df.loc[:, 'buy_order'].groupby(df.index // n).sum()\n",
    "    df1['sell_order'] = df.loc[:, 'sell_order'].groupby(df.index // n).sum()\n",
    "    \n",
    "    #df1['wprice'] = ((df.loc[:, 'wprice'] * df.loc[:, 'base_volume']).groupby(df.index // n).sum() / \n",
    "    #                 df.loc[:, 'base_volume'].groupby(df.index // n).sum()).apply(lambda x: float(x.quantize(Decimal(10) ** -8)))\n",
    "    \n",
    "    return df1\n",
    "\n",
    "def vectorize(array):\n",
    "    \n",
    "    return np.transpose(array).reshape(1, array.size)\n",
    "\n",
    "def unvectorize(vector, x, y):\n",
    "    \n",
    "    return vector.reshape(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_actions,\n",
    "                 checkpoint_path=None):\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        \n",
    "        self.count_states = 0\n",
    "        self.count_episodes = 0\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "#         model.add(Dense(units=120, input_dim=64, activation='relu'))\n",
    "#         model.add(Dense(units=60, activation='relu'))\n",
    "#         model.add(Dense(units=20, activation='relu'))\n",
    "#         model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "        model.add(Dense(units=12, input_dim=9, activation='relu'))\n",
    "        model.add(Dense(units=3, activation='softmax'))\n",
    "        \n",
    "        # Optimizer: adam, RMSProp\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def save(self):\n",
    "        \n",
    "        save_file = 'network_S{}_E{}.h5'.format(self.count_states, self.count_episodes)\n",
    "        model.save(join(self.checkpoint_path, save_file))\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        \n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        # TODO: load count_states and count_episodes from file\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \n",
    "        return [layer.get_weights() for layer in self.model.layers]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \n",
    "        for layer, new_weights in zip(self.model.layers, weights):\n",
    "            layer.set_weights(new_weights)\n",
    "    \n",
    "    def interpolate_weights(self, weights, interpolation_factor=0.001):\n",
    "        \n",
    "        for layer, new_weights in zip(self.model.layers, weights):\n",
    "            layer.set_weights([w1 * interpolation_factor + (1 - interpolation_factor) * w0 for w0, w1 in zip(layer.get_weights(), new_weights)])\n",
    "    \n",
    "    # TODO: add initialization function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Rules:\n",
    "    - Position = 0 -> HOLD, BUY\n",
    "    - Position => 1 -> HOLD, BUY, SELL\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        \n",
    "        self.num_actions = parameters.num_actions\n",
    "        self.discount_factor = parameters.discount_factor\n",
    "        self.epsilon = parameters.epsilon\n",
    "        self.epsilon_min = parameters.epsilon_min\n",
    "        self.epsilon_decay = parameters.epsilon_decay\n",
    "        self.batch_size = parameters.batch_size\n",
    "        \n",
    "        self._initialize()\n",
    "        \n",
    "    def _initialize(self, model_path=None):\n",
    "        \n",
    "        self.train_model = NeuralNetwork(num_actions=self.num_actions)\n",
    "        self.target_model = NeuralNetwork(num_actions=self.num_actions)\n",
    "        \n",
    "        if model_path:\n",
    "            pass\n",
    "            # TODO: implement load model from folder (network_weights, replay_memory, additional_params)\n",
    "        else:\n",
    "            self.replay_memory = ReplayMemory(memory_size=parameters.replay_memory_size,\n",
    "                                              state_shape=parameters.state_length,\n",
    "                                              num_actions = self.num_actions,\n",
    "                                              discount_factor=self.discount_factor)\n",
    "            \n",
    "            self.train_model.build()\n",
    "            self.target_model.build()\n",
    "            \n",
    "            self.target_model.set_weights(self.train_model.get_weights())\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        # no position -> actions allowed: HOLD, BUY\n",
    "        # active position -> HOLD, BUY, SELL\n",
    "        \n",
    "        buy_price = state[0, -1]\n",
    "        \n",
    "        if buy_price == 0:\n",
    "            actions = [0, 1]\n",
    "        else:\n",
    "            actions = [0, 2]\n",
    "            \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return actions[np.random.randint(2)]\n",
    "        \n",
    "        values = self.train_model.model.predict(state)\n",
    "        \n",
    "        return actions[np.argmax(values[0][actions])]\n",
    "    \n",
    "    def replay(self):\n",
    "        \n",
    "        samples = self.replay_memory.random_batch(self.batch_size)\n",
    "        states_batch, action_batch, reward_batch, next_states_batch = map(np.array, samples)\n",
    "\n",
    "        q_values_next = self.train_model.model.predict(next_states_batch)\n",
    "        best_actions = np.argmax(q_values_next, axis=1)\n",
    "        q_values_next_target = self.target_model.model.predict(next_states_batch)\n",
    "        targets_batch = reward_batch + self.discount_factor * np.repeat(q_values_next_target[np.arange(self.batch_size), best_actions].reshape(self.batch_size, 1), self.num_actions, axis=1)\n",
    "        \n",
    "        # TODO: clip reward\n",
    "        # TODO: set target q-value for actions that can't be taken in the current state to 0\n",
    "        \n",
    "        self.train_model.model.fit(states_batch, targets_batch)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def initialize_replay_memory(self, env, parameters, price_index):\n",
    "        \n",
    "        print('Initializing replay memory...')\n",
    "        \n",
    "        state = env.reset()\n",
    "        \n",
    "        for i in range(parameters.replay_memory_size):\n",
    "\n",
    "            action = self.act(state)\n",
    "            next_state, reward, _ = env.step(action, price_index)\n",
    "            self.replay_memory.add(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            \n",
    "        print('Replay memory initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, memory_size, state_shape, num_actions, discount_factor):\n",
    "        \n",
    "        self.memory_size = memory_size\n",
    "        self.state_shape = state_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.states = np.zeros(shape=[memory_size] + state_shape, dtype=np.float64)\n",
    "        self.states_next = np.zeros(shape=[memory_size] + state_shape, dtype=np.float64)\n",
    "        self.actions = np.zeros(shape=[memory_size, num_actions], dtype=np.int8)\n",
    "        self.rewards = np.zeros(shape=[memory_size, num_actions], dtype=np.float64)\n",
    "        \n",
    "        self.indexes = cycle(range(memory_size))\n",
    "        self.pointer = next(self.indexes)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state):\n",
    "        \n",
    "        k = self.pointer\n",
    "\n",
    "        self.states[k, :] = state\n",
    "        self.actions[k] = action\n",
    "        self.rewards[k] = reward  # TODO: consider clipping\n",
    "        self.states_next[k, :] = next_state\n",
    "        \n",
    "        self.pointer = next(self.indexes)\n",
    "        \n",
    "    def random_batch(self, batch_size):\n",
    "        \n",
    "        idx = np.random.randint(self.memory_size, size=batch_size)\n",
    "        \n",
    "        return self.states[idx], self.actions[idx], self.rewards[idx], self.states_next[idx]\n",
    "    \n",
    "    # TODO: implement prioritized state-action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_data,\n",
    "                 steps=None):\n",
    "        \n",
    "        self.steps = steps\n",
    "        self._load(input_data)\n",
    "        \n",
    "        self.episode = {}\n",
    "        \n",
    "    def _load(self, input_data):\n",
    "        \n",
    "        self.data = input_data\n",
    "        \n",
    "        self.shape = input_data.shape\n",
    "        \n",
    "        if self.steps is None or self.steps > self.shape[0]:\n",
    "            self.steps = self.shape[0] - 1\n",
    "    \n",
    "    def _seed(self):\n",
    "        \n",
    "        seed()\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self._seed()\n",
    "        self.episode['start'] = 0\n",
    "        self.episode['current_index'] = self.episode.get('start')\n",
    "        self.data[:, :, -1] = 0  # Reset buy state\n",
    "        \n",
    "        return self.data[self.episode.get('current_index')]\n",
    "        \n",
    "    def step(self, action, price_index):\n",
    "        \n",
    "        current_index = self.episode.get('current_index')\n",
    "        state = self.data[current_index]\n",
    "        next_state = self.data[current_index + 1]\n",
    "\n",
    "        price = state[0, price_index]\n",
    "        buy_price = state[0, -1]\n",
    "        \n",
    "        if action == 1:\n",
    "            next_state[0, -1] = price\n",
    "            reward = [0, price * -2, 0]\n",
    "        elif action == 2:\n",
    "            next_state[0, -1] = 0\n",
    "            margin = price / buy_price\n",
    "            reward = [0, 0, (margin if margin > 1 else -margin) * 2]\n",
    "        else:\n",
    "            next_state[0, -1] = buy_price\n",
    "            reward = [0, 0, 0]\n",
    "        \n",
    "        self.episode['current_index'] += 1\n",
    "        \n",
    "        return next_state, reward, current_index \n",
    "        \n",
    "    def start_new_episode(self):\n",
    "        \n",
    "        self.episode['start'] = np.random.randint(0, self.shape[0] - self.steps)\n",
    "        self.episode['current_index'] = self.episode.get('start')\n",
    "        \n",
    "        return self.data[self.episode.get('current_index')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "\n",
    "    HOLD = 0\n",
    "    BUY = 1\n",
    "    SELL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Summary:\n",
    "    \n",
    "    def __init__(self, parameters=None):\n",
    "        \n",
    "        self.episodes = []\n",
    "        self.params = parameters\n",
    "        \n",
    "    def create(self, actions, train_reward, actual_reward, start, end, epsilon_end):\n",
    "        \n",
    "        # TODO: add epsilon values\n",
    "        \n",
    "        self.episodes.append({\n",
    "            'actions': actions,\n",
    "            'train_reward': train_reward,\n",
    "            'actual_reward': actual_reward,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'epsilon_end': epsilon_end\n",
    "        })\n",
    "        \n",
    "    def summarize_all(self):\n",
    "        \n",
    "        num_episodes = len(self.episodes)\n",
    "        train_reward = np.zeros(num_episodes)\n",
    "        actual_reward = np.zeros(num_episodes)\n",
    "        epsilon_end = np.zeros(num_episodes)\n",
    "        \n",
    "        for ii in range(num_episodes):\n",
    "            train_reward[ii], actual_reward[ii], epsilon_end[ii] = (self.episodes[ii][x] for x in ['train_reward', 'actual_reward', 'epsilon_end'])\n",
    "        \n",
    "        # TODO: add plot here\n",
    "        \n",
    "        return train_reward, actual_reward, epsilon_end\n",
    "    \n",
    "    def summarize_episode(self, episode_num):\n",
    "        \n",
    "        episode_summary = self.episodes[episode_num]\n",
    "        \n",
    "        return episode_summary\n",
    "    \n",
    "    def get_price(self, price, episode_num):\n",
    "        \n",
    "        start = self.episodes[episode_num].get('start')\n",
    "        end = self.episodes[episode_num].get('end')\n",
    "        \n",
    "        return price[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data processing parameters\n",
    "n = 15\n",
    "m = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "x = np.linspace(-np.pi, np.pi, 192)\n",
    "array = np.sin(x)\n",
    "\n",
    "price_state_index = -2\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "normalized_array = min_max_scaler.fit_transform(array.reshape(-1, 1))\n",
    "\n",
    "# Build states\n",
    "states = np.array([vectorize(normalized_array[x:x+m, :]) for x in range(normalized_array.shape[0] - m + 1)])\n",
    "\n",
    "# Add state for position held\n",
    "states = np.insert(states, states.shape[2], 0, axis=2)\n",
    "\n",
    "# Build prices\n",
    "prices = list(map(float, array[(m-1):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine data\n",
    "df = time_transform(data.iloc[:200*15,:], n)\n",
    "array = df.iloc[:,1:].as_matrix()\n",
    "\n",
    "# Get close price column index\n",
    "price_df_index = df.columns.get_loc('close')\n",
    "price_array_index = price_df_index-1\n",
    "price_state_index = m*(price_array_index+1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhsu/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Normalize data \n",
    "# TODO: change normalization if necessary\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "normalized_array = min_max_scaler.fit_transform(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-0ccc858e7fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Build prices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_array_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Build states\n",
    "states = np.array([vectorize(normalized_array[x:x+m, :]) for x in range(normalized_array.shape[0] - m + 1)])\n",
    "\n",
    "# Add state for position\n",
    "states = np.insert(states, states.shape[2], 0, axis=2)\n",
    "\n",
    "# Build prices\n",
    "prices = list(map(float, array[(m-1):, price_array_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185, 185)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states), len(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Time series parameters\n",
    "        self.num_actions = 3\n",
    "        \n",
    "        # Neural network parameters\n",
    "        self.discount_factor = 0.97\n",
    "        self.epsilon = 0.99\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.98\n",
    "        \n",
    "        # Replay memory parameters\n",
    "        self.replay_memory_size = 184\n",
    "        self.batch_size = 100\n",
    "        \n",
    "        # Training parameters\n",
    "        self.episodes = 100\n",
    "        self.episode_length = 184\n",
    "        self.update_target_weights_step_size = 100\n",
    "        self.train_model_step_size = 100\n",
    "        self.interpolation_factor = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing replay memory...\n",
      "Replay memory initialized.\n",
      "Step 0: Updating target Q network weights with latest Q network weights.\n",
      "Step 0: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.7495\n",
      "Step 100: Updating target Q network weights with latest Q network weights.\n",
      "Step 100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 138us/step - loss: 0.6045\n",
      "Step 200: Updating target Q network weights with latest Q network weights.\n",
      "Step 200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 75us/step - loss: 0.5398\n",
      "Step 300: Updating target Q network weights with latest Q network weights.\n",
      "Step 300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 179us/step - loss: 0.4532\n",
      "Step 400: Updating target Q network weights with latest Q network weights.\n",
      "Step 400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 111us/step - loss: 0.4720\n",
      "Step 500: Updating target Q network weights with latest Q network weights.\n",
      "Step 500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.330 - 0s 111us/step - loss: 0.6985\n",
      "Step 600: Updating target Q network weights with latest Q network weights.\n",
      "Step 600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 293us/step - loss: 1.0197\n",
      "Step 700: Updating target Q network weights with latest Q network weights.\n",
      "Step 700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 107us/step - loss: 0.5431\n",
      "Step 800: Updating target Q network weights with latest Q network weights.\n",
      "Step 800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 203us/step - loss: 0.6952\n",
      "Step 900: Updating target Q network weights with latest Q network weights.\n",
      "Step 900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 335us/step - loss: 0.6092\n",
      "Step 1000: Updating target Q network weights with latest Q network weights.\n",
      "Step 1000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 118us/step - loss: 0.7397\n",
      "Step 1100: Updating target Q network weights with latest Q network weights.\n",
      "Step 1100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.5730\n",
      "Step 1200: Updating target Q network weights with latest Q network weights.\n",
      "Step 1200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 211us/step - loss: 1.2347\n",
      "Step 1300: Updating target Q network weights with latest Q network weights.\n",
      "Step 1300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 76us/step - loss: 1.1298\n",
      "Step 1400: Updating target Q network weights with latest Q network weights.\n",
      "Step 1400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.5854\n",
      "Step 1500: Updating target Q network weights with latest Q network weights.\n",
      "Step 1500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 105us/step - loss: 0.4457\n",
      "Step 1600: Updating target Q network weights with latest Q network weights.\n",
      "Step 1600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 102us/step - loss: 0.5137\n",
      "Step 1700: Updating target Q network weights with latest Q network weights.\n",
      "Step 1700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 189us/step - loss: 0.3849\n",
      "Step 1800: Updating target Q network weights with latest Q network weights.\n",
      "Step 1800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 87us/step - loss: 0.4103\n",
      "Step 1900: Updating target Q network weights with latest Q network weights.\n",
      "Step 1900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 122us/step - loss: 0.3683\n",
      "Step 2000: Updating target Q network weights with latest Q network weights.\n",
      "Step 2000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 87us/step - loss: 0.4577\n",
      "Step 2100: Updating target Q network weights with latest Q network weights.\n",
      "Step 2100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 136us/step - loss: 0.5096\n",
      "Step 2200: Updating target Q network weights with latest Q network weights.\n",
      "Step 2200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.6814\n",
      "Step 2300: Updating target Q network weights with latest Q network weights.\n",
      "Step 2300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 92us/step - loss: 0.4892\n",
      "Step 2400: Updating target Q network weights with latest Q network weights.\n",
      "Step 2400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 182us/step - loss: 0.4626\n",
      "Step 2500: Updating target Q network weights with latest Q network weights.\n",
      "Step 2500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 81us/step - loss: 0.3614\n",
      "Step 2600: Updating target Q network weights with latest Q network weights.\n",
      "Step 2600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 101us/step - loss: 0.3945\n",
      "Step 2700: Updating target Q network weights with latest Q network weights.\n",
      "Step 2700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 86us/step - loss: 0.3246\n",
      "Step 2800: Updating target Q network weights with latest Q network weights.\n",
      "Step 2800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 77us/step - loss: 0.4072\n",
      "Step 2900: Updating target Q network weights with latest Q network weights.\n",
      "Step 2900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 184us/step - loss: 0.3645\n",
      "Step 3000: Updating target Q network weights with latest Q network weights.\n",
      "Step 3000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 85us/step - loss: 0.3848\n",
      "Step 3100: Updating target Q network weights with latest Q network weights.\n",
      "Step 3100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.265 - 0s 98us/step - loss: 0.2749\n",
      "Step 3200: Updating target Q network weights with latest Q network weights.\n",
      "Step 3200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 106us/step - loss: 0.2934\n",
      "Step 3300: Updating target Q network weights with latest Q network weights.\n",
      "Step 3300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 152us/step - loss: 0.2410\n",
      "Step 3400: Updating target Q network weights with latest Q network weights.\n",
      "Step 3400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 127us/step - loss: 1.2108\n",
      "Step 3500: Updating target Q network weights with latest Q network weights.\n",
      "Step 3500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 78us/step - loss: 1.1917\n",
      "Step 3600: Updating target Q network weights with latest Q network weights.\n",
      "Step 3600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 107us/step - loss: 0.3088\n",
      "Step 3700: Updating target Q network weights with latest Q network weights.\n",
      "Step 3700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 146us/step - loss: 223.9300\n",
      "Step 3800: Updating target Q network weights with latest Q network weights.\n",
      "Step 3800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.4268\n",
      "Step 3900: Updating target Q network weights with latest Q network weights.\n",
      "Step 3900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 150us/step - loss: 1.1462\n",
      "Step 4000: Updating target Q network weights with latest Q network weights.\n",
      "Step 4000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 335us/step - loss: 0.4563\n",
      "Step 4100: Updating target Q network weights with latest Q network weights.\n",
      "Step 4100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 98us/step - loss: 0.4277\n",
      "Step 4200: Updating target Q network weights with latest Q network weights.\n",
      "Step 4200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 182us/step - loss: 0.3543\n",
      "Step 4300: Updating target Q network weights with latest Q network weights.\n",
      "Step 4300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.2828\n",
      "Step 4400: Updating target Q network weights with latest Q network weights.\n",
      "Step 4400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 139us/step - loss: 0.2737\n",
      "Step 4500: Updating target Q network weights with latest Q network weights.\n",
      "Step 4500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.234 - 0s 146us/step - loss: 0.2875\n",
      "Step 4600: Updating target Q network weights with latest Q network weights.\n",
      "Step 4600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.1923\n",
      "Step 4700: Updating target Q network weights with latest Q network weights.\n",
      "Step 4700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.2855\n",
      "Step 4800: Updating target Q network weights with latest Q network weights.\n",
      "Step 4800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2929\n",
      "Step 4900: Updating target Q network weights with latest Q network weights.\n",
      "Step 4900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 107us/step - loss: 0.2803\n",
      "Step 5000: Updating target Q network weights with latest Q network weights.\n",
      "Step 5000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.4925\n",
      "Step 5100: Updating target Q network weights with latest Q network weights.\n",
      "Step 5100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 103us/step - loss: 0.2614\n",
      "Step 5200: Updating target Q network weights with latest Q network weights.\n",
      "Step 5200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 163us/step - loss: 0.3398\n",
      "Step 5300: Updating target Q network weights with latest Q network weights.\n",
      "Step 5300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 129us/step - loss: 0.3396\n",
      "Step 5400: Updating target Q network weights with latest Q network weights.\n",
      "Step 5400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 136us/step - loss: 0.2439\n",
      "Step 5500: Updating target Q network weights with latest Q network weights.\n",
      "Step 5500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3492\n",
      "Step 5600: Updating target Q network weights with latest Q network weights.\n",
      "Step 5600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 87us/step - loss: 0.1765\n",
      "Step 5700: Updating target Q network weights with latest Q network weights.\n",
      "Step 5700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 241us/step - loss: 0.1735\n",
      "Step 5800: Updating target Q network weights with latest Q network weights.\n",
      "Step 5800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 83us/step - loss: 0.5966\n",
      "Step 5900: Updating target Q network weights with latest Q network weights.\n",
      "Step 5900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.2362\n",
      "Step 6000: Updating target Q network weights with latest Q network weights.\n",
      "Step 6000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 77us/step - loss: 0.2312\n",
      "Step 6100: Updating target Q network weights with latest Q network weights.\n",
      "Step 6100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 111us/step - loss: 0.3623\n",
      "Step 6200: Updating target Q network weights with latest Q network weights.\n",
      "Step 6200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 83us/step - loss: 0.3429\n",
      "Step 6300: Updating target Q network weights with latest Q network weights.\n",
      "Step 6300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 136us/step - loss: 0.0675\n",
      "Step 6400: Updating target Q network weights with latest Q network weights.\n",
      "Step 6400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 85us/step - loss: 0.1461\n",
      "Step 6500: Updating target Q network weights with latest Q network weights.\n",
      "Step 6500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.1145\n",
      "Step 6600: Updating target Q network weights with latest Q network weights.\n",
      "Step 6600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 103us/step - loss: 0.1015\n",
      "Step 6700: Updating target Q network weights with latest Q network weights.\n",
      "Step 6700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 75us/step - loss: 0.0769\n",
      "Step 6800: Updating target Q network weights with latest Q network weights.\n",
      "Step 6800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 105us/step - loss: 0.0534\n",
      "Step 6900: Updating target Q network weights with latest Q network weights.\n",
      "Step 6900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 91us/step - loss: 0.2020\n",
      "Step 7000: Updating target Q network weights with latest Q network weights.\n",
      "Step 7000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.2025\n",
      "Step 7100: Updating target Q network weights with latest Q network weights.\n",
      "Step 7100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.1091\n",
      "Step 7200: Updating target Q network weights with latest Q network weights.\n",
      "Step 7200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.1585\n",
      "Step 7300: Updating target Q network weights with latest Q network weights.\n",
      "Step 7300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.0976\n",
      "Step 7400: Updating target Q network weights with latest Q network weights.\n",
      "Step 7400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 82us/step - loss: 3265.3715\n",
      "Step 7500: Updating target Q network weights with latest Q network weights.\n",
      "Step 7500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 86us/step - loss: 0.0703\n",
      "Step 7600: Updating target Q network weights with latest Q network weights.\n",
      "Step 7600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.1633\n",
      "Step 7700: Updating target Q network weights with latest Q network weights.\n",
      "Step 7700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 81us/step - loss: 0.1904\n",
      "Step 7800: Updating target Q network weights with latest Q network weights.\n",
      "Step 7800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.0786\n",
      "Step 7900: Updating target Q network weights with latest Q network weights.\n",
      "Step 7900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.1111\n",
      "Step 8000: Updating target Q network weights with latest Q network weights.\n",
      "Step 8000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.1921\n",
      "Step 8100: Updating target Q network weights with latest Q network weights.\n",
      "Step 8100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.1656\n",
      "Step 8200: Updating target Q network weights with latest Q network weights.\n",
      "Step 8200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 161us/step - loss: 0.2126\n",
      "Step 8300: Updating target Q network weights with latest Q network weights.\n",
      "Step 8300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 157us/step - loss: 0.1794\n",
      "Step 8400: Updating target Q network weights with latest Q network weights.\n",
      "Step 8400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.1129\n",
      "Step 8500: Updating target Q network weights with latest Q network weights.\n",
      "Step 8500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.2200\n",
      "Step 8600: Updating target Q network weights with latest Q network weights.\n",
      "Step 8600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.0675\n",
      "Step 8700: Updating target Q network weights with latest Q network weights.\n",
      "Step 8700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.0568\n",
      "Step 8800: Updating target Q network weights with latest Q network weights.\n",
      "Step 8800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.4313\n",
      "Step 8900: Updating target Q network weights with latest Q network weights.\n",
      "Step 8900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 93us/step - loss: 0.6308\n",
      "Step 9000: Updating target Q network weights with latest Q network weights.\n",
      "Step 9000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.4950\n",
      "Step 9100: Updating target Q network weights with latest Q network weights.\n",
      "Step 9100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 105us/step - loss: 0.1108\n",
      "Step 9200: Updating target Q network weights with latest Q network weights.\n",
      "Step 9200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 77us/step - loss: 0.0394\n",
      "Step 9300: Updating target Q network weights with latest Q network weights.\n",
      "Step 9300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 117us/step - loss: 0.1177\n",
      "Step 9400: Updating target Q network weights with latest Q network weights.\n",
      "Step 9400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.1111\n",
      "Step 9500: Updating target Q network weights with latest Q network weights.\n",
      "Step 9500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 2.3906\n",
      "Step 9600: Updating target Q network weights with latest Q network weights.\n",
      "Step 9600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.0722\n",
      "Step 9700: Updating target Q network weights with latest Q network weights.\n",
      "Step 9700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.0045\n",
      "Step 9800: Updating target Q network weights with latest Q network weights.\n",
      "Step 9800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.0169\n",
      "Step 9900: Updating target Q network weights with latest Q network weights.\n",
      "Step 9900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.0453\n",
      "Step 10000: Updating target Q network weights with latest Q network weights.\n",
      "Step 10000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 91us/step - loss: 0.3081\n",
      "Step 10100: Updating target Q network weights with latest Q network weights.\n",
      "Step 10100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.4000\n",
      "Step 10200: Updating target Q network weights with latest Q network weights.\n",
      "Step 10200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.0494\n",
      "Step 10300: Updating target Q network weights with latest Q network weights.\n",
      "Step 10300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 92us/step - loss: 0.0055\n",
      "Step 10400: Updating target Q network weights with latest Q network weights.\n",
      "Step 10400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.0358\n",
      "Step 10500: Updating target Q network weights with latest Q network weights.\n",
      "Step 10500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 103us/step - loss: 0.0172\n",
      "Step 10600: Updating target Q network weights with latest Q network weights.\n",
      "Step 10600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 91us/step - loss: 0.0843\n",
      "Step 10700: Updating target Q network weights with latest Q network weights.\n",
      "Step 10700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 106us/step - loss: 0.3113\n",
      "Step 10800: Updating target Q network weights with latest Q network weights.\n",
      "Step 10800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.1958\n",
      "Step 10900: Updating target Q network weights with latest Q network weights.\n",
      "Step 10900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 65us/step - loss: 42.6852\n",
      "Step 11000: Updating target Q network weights with latest Q network weights.\n",
      "Step 11000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.1345\n",
      "Step 11100: Updating target Q network weights with latest Q network weights.\n",
      "Step 11100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.0599\n",
      "Step 11200: Updating target Q network weights with latest Q network weights.\n",
      "Step 11200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 141us/step - loss: 0.1743\n",
      "Step 11300: Updating target Q network weights with latest Q network weights.\n",
      "Step 11300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 75us/step - loss: 0.1427\n",
      "Step 11400: Updating target Q network weights with latest Q network weights.\n",
      "Step 11400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 116us/step - loss: 0.0490\n",
      "Step 11500: Updating target Q network weights with latest Q network weights.\n",
      "Step 11500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.1855\n",
      "Step 11600: Updating target Q network weights with latest Q network weights.\n",
      "Step 11600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.2167\n",
      "Step 11700: Updating target Q network weights with latest Q network weights.\n",
      "Step 11700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.0203\n",
      "Step 11800: Updating target Q network weights with latest Q network weights.\n",
      "Step 11800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 81us/step - loss: 0.0924\n",
      "Step 11900: Updating target Q network weights with latest Q network weights.\n",
      "Step 11900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.0834\n",
      "Step 12000: Updating target Q network weights with latest Q network weights.\n",
      "Step 12000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.0689\n",
      "Step 12100: Updating target Q network weights with latest Q network weights.\n",
      "Step 12100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.0261\n",
      "Step 12200: Updating target Q network weights with latest Q network weights.\n",
      "Step 12200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.1854\n",
      "Step 12300: Updating target Q network weights with latest Q network weights.\n",
      "Step 12300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.2075\n",
      "Step 12400: Updating target Q network weights with latest Q network weights.\n",
      "Step 12400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.1150\n",
      "Step 12500: Updating target Q network weights with latest Q network weights.\n",
      "Step 12500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 76us/step - loss: 0.1362\n",
      "Step 12600: Updating target Q network weights with latest Q network weights.\n",
      "Step 12600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 102us/step - loss: 0.0841\n",
      "Step 12700: Updating target Q network weights with latest Q network weights.\n",
      "Step 12700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 78us/step - loss: 0.2057\n",
      "Step 12800: Updating target Q network weights with latest Q network weights.\n",
      "Step 12800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 76us/step - loss: 0.0607\n",
      "Step 12900: Updating target Q network weights with latest Q network weights.\n",
      "Step 12900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 85us/step - loss: 0.0238\n",
      "Step 13000: Updating target Q network weights with latest Q network weights.\n",
      "Step 13000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.0325\n",
      "Step 13100: Updating target Q network weights with latest Q network weights.\n",
      "Step 13100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 86us/step - loss: 0.0172\n",
      "Step 13200: Updating target Q network weights with latest Q network weights.\n",
      "Step 13200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 83us/step - loss: 0.0039\n",
      "Step 13300: Updating target Q network weights with latest Q network weights.\n",
      "Step 13300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 92us/step - loss: 0.0269\n",
      "Step 13400: Updating target Q network weights with latest Q network weights.\n",
      "Step 13400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.1171\n",
      "Step 13500: Updating target Q network weights with latest Q network weights.\n",
      "Step 13500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.0203\n",
      "Step 13600: Updating target Q network weights with latest Q network weights.\n",
      "Step 13600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 75us/step - loss: 0.0823\n",
      "Step 13700: Updating target Q network weights with latest Q network weights.\n",
      "Step 13700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.0056\n",
      "Step 13800: Updating target Q network weights with latest Q network weights.\n",
      "Step 13800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 81us/step - loss: 0.1415\n",
      "Step 13900: Updating target Q network weights with latest Q network weights.\n",
      "Step 13900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.0648\n",
      "Step 14000: Updating target Q network weights with latest Q network weights.\n",
      "Step 14000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 159us/step - loss: 0.0346\n",
      "Step 14100: Updating target Q network weights with latest Q network weights.\n",
      "Step 14100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.0669\n",
      "Step 14200: Updating target Q network weights with latest Q network weights.\n",
      "Step 14200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 91us/step - loss: 0.0175\n",
      "Step 14300: Updating target Q network weights with latest Q network weights.\n",
      "Step 14300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 83us/step - loss: 0.1271\n",
      "Step 14400: Updating target Q network weights with latest Q network weights.\n",
      "Step 14400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 82us/step - loss: 0.1090\n",
      "Step 14500: Updating target Q network weights with latest Q network weights.\n",
      "Step 14500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.0535\n",
      "Step 14600: Updating target Q network weights with latest Q network weights.\n",
      "Step 14600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 83us/step - loss: 0.0940\n",
      "Step 14700: Updating target Q network weights with latest Q network weights.\n",
      "Step 14700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.0147\n",
      "Step 14800: Updating target Q network weights with latest Q network weights.\n",
      "Step 14800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 92us/step - loss: 0.0283\n",
      "Step 14900: Updating target Q network weights with latest Q network weights.\n",
      "Step 14900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 76us/step - loss: 0.0912\n",
      "Step 15000: Updating target Q network weights with latest Q network weights.\n",
      "Step 15000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.2114\n",
      "Step 15100: Updating target Q network weights with latest Q network weights.\n",
      "Step 15100: Sampling replay memory and training Q network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.0933\n",
      "Step 15200: Updating target Q network weights with latest Q network weights.\n",
      "Step 15200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 102us/step - loss: 0.1003\n",
      "Step 15300: Updating target Q network weights with latest Q network weights.\n",
      "Step 15300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.0400\n",
      "Step 15400: Updating target Q network weights with latest Q network weights.\n",
      "Step 15400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.0696\n",
      "Step 15500: Updating target Q network weights with latest Q network weights.\n",
      "Step 15500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.0747\n",
      "Step 15600: Updating target Q network weights with latest Q network weights.\n",
      "Step 15600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.0383\n",
      "Step 15700: Updating target Q network weights with latest Q network weights.\n",
      "Step 15700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.0152\n",
      "Step 15800: Updating target Q network weights with latest Q network weights.\n",
      "Step 15800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.0873\n",
      "Step 15900: Updating target Q network weights with latest Q network weights.\n",
      "Step 15900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 87us/step - loss: 0.0785\n",
      "Step 16000: Updating target Q network weights with latest Q network weights.\n",
      "Step 16000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.0392\n",
      "Step 16100: Updating target Q network weights with latest Q network weights.\n",
      "Step 16100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.0422\n",
      "Step 16200: Updating target Q network weights with latest Q network weights.\n",
      "Step 16200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 81us/step - loss: 0.1001\n",
      "Step 16300: Updating target Q network weights with latest Q network weights.\n",
      "Step 16300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 103us/step - loss: 0.1648\n",
      "Step 16400: Updating target Q network weights with latest Q network weights.\n",
      "Step 16400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.0519\n",
      "Step 16500: Updating target Q network weights with latest Q network weights.\n",
      "Step 16500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.0763\n",
      "Step 16600: Updating target Q network weights with latest Q network weights.\n",
      "Step 16600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.0872\n",
      "Step 16700: Updating target Q network weights with latest Q network weights.\n",
      "Step 16700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.0888\n",
      "Step 16800: Updating target Q network weights with latest Q network weights.\n",
      "Step 16800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.0157\n",
      "Step 16900: Updating target Q network weights with latest Q network weights.\n",
      "Step 16900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.0362\n",
      "Step 17000: Updating target Q network weights with latest Q network weights.\n",
      "Step 17000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.0629\n",
      "Step 17100: Updating target Q network weights with latest Q network weights.\n",
      "Step 17100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.0723\n",
      "Step 17200: Updating target Q network weights with latest Q network weights.\n",
      "Step 17200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 95us/step - loss: 0.0762\n",
      "Step 17300: Updating target Q network weights with latest Q network weights.\n",
      "Step 17300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.0439\n",
      "Step 17400: Updating target Q network weights with latest Q network weights.\n",
      "Step 17400: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.0557\n",
      "Step 17500: Updating target Q network weights with latest Q network weights.\n",
      "Step 17500: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.0365\n",
      "Step 17600: Updating target Q network weights with latest Q network weights.\n",
      "Step 17600: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.0794\n",
      "Step 17700: Updating target Q network weights with latest Q network weights.\n",
      "Step 17700: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.0909\n",
      "Step 17800: Updating target Q network weights with latest Q network weights.\n",
      "Step 17800: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 78us/step - loss: 0.0785\n",
      "Step 17900: Updating target Q network weights with latest Q network weights.\n",
      "Step 17900: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 75us/step - loss: 0.0893\n",
      "Step 18000: Updating target Q network weights with latest Q network weights.\n",
      "Step 18000: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.0369\n",
      "Step 18100: Updating target Q network weights with latest Q network weights.\n",
      "Step 18100: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.0279\n",
      "Step 18200: Updating target Q network weights with latest Q network weights.\n",
      "Step 18200: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 97us/step - loss: 0.1107\n",
      "Step 18300: Updating target Q network weights with latest Q network weights.\n",
      "Step 18300: Sampling replay memory and training Q network.\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 97us/step - loss: 0.0504\n"
     ]
    }
   ],
   "source": [
    "## Model training\n",
    "\n",
    "parameters = Parameters()\n",
    "parameters.state_length = [states.shape[-1]]\n",
    "agent = Agent(parameters)\n",
    "env = Environment(states, steps=parameters.episode_length)\n",
    "summary = Summary()\n",
    "step = 0\n",
    "\n",
    "# Populate the replay memory with initial experiences\n",
    "agent.initialize_replay_memory(env, parameters, price_index=price_state_index)\n",
    "\n",
    "for e in range(parameters.episodes):\n",
    "    \n",
    "    state = env.start_new_episode()\n",
    "    \n",
    "    episode_learn_reward = 0\n",
    "    episode_actions = np.zeros(parameters.episode_length)\n",
    "    episode_actual_reward = 0\n",
    "    \n",
    "    for ii in range(parameters.episode_length):\n",
    "        \n",
    "        # Update the target network weights every X iterations\n",
    "        if step % parameters.update_target_weights_step_size == 0:\n",
    "            print('Step {}: Updating target Q network weights with latest Q network weights.'.format(step))\n",
    "            agent.target_model.interpolate_weights(agent.train_model.get_weights(), parameters.interpolation_factor)\n",
    "                \n",
    "        action = agent.act(state)\n",
    "        next_state, reward, index = env.step(action, price_index=price_state_index)\n",
    "        agent.replay_memory.add(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        episode_actions[ii] = action\n",
    "        episode_learn_reward += reward[action]\n",
    "        \n",
    "        if action > 0:\n",
    "            price = prices[index]\n",
    "            episode_actual_reward += price * -1.0025 if action == 1 else price * 0.9975\n",
    "        \n",
    "        if step % parameters.train_model_step_size == 0:\n",
    "            print('Step {}: Sampling replay memory and training Q network.'.format(step))\n",
    "            \n",
    "            agent.replay()\n",
    "            \n",
    "        step += 1\n",
    "    \n",
    "    summary.create(episode_actions, episode_learn_reward, episode_actual_reward, env.episode.get('start'), env.episode.get('current_index')+1, agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: 3.1866214400958928\n",
      "Actual reward: 0.15811827603613826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a2660dda0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1dbe4b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine episode\n",
    "\n",
    "episode_num = 99\n",
    "\n",
    "episode_summary = summary.summarize_episode(episode_num)\n",
    "episode_price = summary.get_price(prices, episode_num)\n",
    "actions = episode_summary.get('actions')\n",
    "\n",
    "print('Training reward: {}'.format(episode_summary.get('train_reward')))\n",
    "print('Actual reward: {}'.format(episode_summary.get('actual_reward')))\n",
    "\n",
    "# Plot graph of buy/sell actions\n",
    "buy_indices = [x for x in range(len(actions)) if actions[x] == 1]\n",
    "buy_prices = [episode_price[x] for x in buy_indices]\n",
    "\n",
    "sell_indices = [x for x in range(len(actions)) if actions[x] == 2]\n",
    "sell_prices = [episode_price[x] for x in sell_indices]\n",
    "\n",
    "plt.plot(episode_price)\n",
    "plt.scatter(buy_indices, buy_prices, color='r')\n",
    "plt.scatter(sell_indices, sell_prices, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, actual, epsilon = summary.summarize_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a220e83c8>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhjjURYAjDHGoywAGGOMR/1/rkXA6ZW2iVAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22001ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
